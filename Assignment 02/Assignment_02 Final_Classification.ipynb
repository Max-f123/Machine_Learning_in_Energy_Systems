{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T21:32:30.222994Z",
     "start_time": "2024-11-22T21:32:30.205986Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, f1_score, matthews_corrcoef, balanced_accuracy_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from sklearn.inspection import DecisionBoundaryDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data. If existing solution for optimization problem shall be used, use the \"results1000\" code. Otherwise use the \n",
    "results = pd.read_csv('Data/results1000.csv', delimiter=',') \n",
    "results_UC = pd.read_csv('Data/unit_comitment1000.csv', delimiter=',')\n",
    "\n",
    "# results = pd.read_csv('Data/resultsopti.csv', delimiter=',') \n",
    "# results_UC = pd.read_csv('Data/unit_comitmentopti.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 03: Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original feature space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T21:33:11.234423Z",
     "start_time": "2024-11-22T21:33:11.064904Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G1    1.000000\n",
      "G2    0.696333\n",
      "G3    0.965833\n",
      "dtype: float64\n",
      "Units to classify: Index(['G2', 'G3'], dtype='object')\n",
      "--------------------------------------------------\n",
      "Shapes:  (16800, 5) (3600, 5) (16800, 2) (3600, 2) (3600, 5) (3600, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Check if units are always ON or OFF\n",
    "targets = results_UC[[\"G1\",\"G2\",\"G3\"]]\n",
    "print(targets.mean())\n",
    "\n",
    "# Filter out units that are always ON or OFF\n",
    "units_to_classify = targets.columns[(targets.mean() > 0) & (targets.mean() < 1)]\n",
    "print(f\"Units to classify: {units_to_classify}\")\n",
    "print(\"-\"*50)\n",
    "targets = targets[units_to_classify]\n",
    "features = results[['wind generation 1', 'wind generation 2', 'load 1', 'load 2', 'load 3']]\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "features = pd.DataFrame(scaler.fit_transform(features), columns=results[['wind generation 1', 'wind generation 2', 'load 1', 'load 2', 'load 3']].columns)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(features, targets, test_size=0.3, random_state=42, shuffle=False)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, shuffle=False)\n",
    "\n",
    "# Shape\n",
    "print(\"Shapes: \", X_train.shape, X_test.shape, y_train.shape, y_test.shape, X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large feature space (incl. lagged features):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T21:33:11.266421Z",
     "start_time": "2024-11-22T21:33:11.238414Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lagging Function\n",
    "def lag_features(features, lags, targets=None, lag_targets=False):\n",
    "    \"\"\"\n",
    "    Creates lagged versions of the features for a given number of lags.\n",
    "    Optionally adds lagged versions of the targets as well.\n",
    "    Parameters:\n",
    "    - features (pd.DataFrame): The input feature DataFrame.\n",
    "    - lags (int): Number of lagged steps to create.\n",
    "    - targets (pd.DataFrame, optional): The target DataFrame to lag.\n",
    "    - lag_targets (bool): Whether to include lagged versions of targets.\n",
    "    \"\"\"\n",
    "    lagged_features = features.copy()\n",
    "    for lag in range(1, lags + 1):\n",
    "        lagged = features.shift(lag)\n",
    "        lagged.columns = [f\"{col}_lag{lag}\" for col in features.columns]\n",
    "        lagged_features = pd.concat([lagged_features, lagged], axis=1)\n",
    "\n",
    "    # Add lagged targets if specified\n",
    "    if lag_targets and targets is not None:\n",
    "        for lag in range(1, lags + 1):\n",
    "            lagged = targets.shift(lag)\n",
    "            lagged.columns = [f\"{col}_lag{lag}\" for col in targets.columns]\n",
    "            lagged_features = pd.concat([lagged_features, lagged], axis=1)\n",
    "    \n",
    "    # Drop rows with NaN values from lagging\n",
    "    return lagged_features.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T21:33:11.297414Z",
     "start_time": "2024-11-22T21:33:11.271420Z"
    }
   },
   "outputs": [],
   "source": [
    "# Multi-Class Target Transformation (nested)\n",
    "def transform_targets_to_multiclass(targets_df):\n",
    "    \"\"\"\n",
    "    Transforms binary targets (G2, G3) into multi-class categories.\n",
    "    \"\"\"\n",
    "    def map_targets(row):\n",
    "        if row['G2'] == 1 and row['G3'] == 1:\n",
    "            return 'A'\n",
    "        elif row['G2'] == 0 and row['G3'] == 1:\n",
    "            return 'B'\n",
    "        elif row['G2'] == 1 and row['G3'] == 0:\n",
    "            return 'C'\n",
    "        elif row['G2'] == 0 and row['G3'] == 0:\n",
    "            return 'D'\n",
    "        else:\n",
    "            raise ValueError(\"Unexpected row values: G2={}, G3={}\".format(row['G2'], row['G3'])) # Catch unexpected values\n",
    "\n",
    "    # Copy targets\n",
    "    targets_MC = targets_df.copy()    \n",
    "    # Apply transformation\n",
    "    targets_MC['category'] = targets_df.apply(map_targets, axis=1)\n",
    "    # Drop original columns\n",
    "    targets_MC = targets_MC.drop(columns=['G2', 'G3'])\n",
    "    # Convert to categorical codes (0, 1, 2, 3)\n",
    "    targets_MC['category'] = pd.Categorical(targets_MC['category']).codes\n",
    "    return targets_MC[['category']]  # Return only the transformed category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T21:33:11.328945Z",
     "start_time": "2024-11-22T21:33:11.304409Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reverse Mapping\n",
    "def reverse_map_categories(categories, add_g1=True):\n",
    "    \"\"\"\n",
    "    Reverse map from multi-class categories to binary G1, G2, G3, ensuring column order.\n",
    "    \"\"\"\n",
    "    reverse_mapping = {\n",
    "        0: (1, 1),  # A\n",
    "        1: (0, 1),  # B\n",
    "        2: (1, 0),  # C\n",
    "        3: (0, 0),  # D\n",
    "    }\n",
    "\n",
    "    if isinstance(categories, (pd.Series, pd.DataFrame)):\n",
    "        mapped = categories.map(reverse_mapping)\n",
    "        result = pd.DataFrame(mapped.tolist(), columns=['G2', 'G3'])\n",
    "        if add_g1:\n",
    "            result.insert(0, 'G1', 1)  # Ensure G1 is the first column\n",
    "        return result[['G1', 'G2', 'G3']] if add_g1 else result[['G2', 'G3']]\n",
    "\n",
    "    result = reverse_mapping.get(categories, (None, None))\n",
    "    return (1, *result) if add_g1 else result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T21:33:11.360952Z",
     "start_time": "2024-11-22T21:33:11.333953Z"
    }
   },
   "outputs": [],
   "source": [
    "# Combine Lagging and Multi-Class Targets\n",
    "def prepare_data(features, targets, lags, lag_targets=False):\n",
    "    \"\"\"\n",
    "    Combines feature lagging and multi-class target transformation.\n",
    "    \"\"\"\n",
    "    # Lag the features\n",
    "    lagged_features = lag_features(features, lags, targets, lag_targets=lag_targets)\n",
    "\n",
    "    # Add hours to lagged features\n",
    "    lagged_features['Hour'] = features.index[lags:]\n",
    "\n",
    "    # Cyclic Scaling of Hour\n",
    "    lagged_features['Hour_sin'] = np.sin(2 * np.pi * lagged_features['Hour'] / 24)\n",
    "    lagged_features['Hour_cos'] = np.cos(2 * np.pi * lagged_features['Hour'] / 24)\n",
    "    lagged_features = lagged_features.drop(columns=['Hour'])\n",
    "    \n",
    "    # Align targets with lagged features\n",
    "    lagged_targets = targets.iloc[lags:].reset_index(drop=True)  # Drop first rows to match lagged features\n",
    "\n",
    "    # Transform targets to multi-class\n",
    "    multiclass_targets = transform_targets_to_multiclass(lagged_targets) #.reset_index(drop=True)\n",
    "\n",
    "    # Ensure alignment\n",
    "    assert lagged_features.shape[0] == multiclass_targets.shape[0], \"Mismatch in rows between features and targets\"\n",
    "    \n",
    "    return lagged_features, multiclass_targets #.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T21:33:11.391949Z",
     "start_time": "2024-11-22T21:33:11.363953Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add a function to evaluate and display performance metrics\n",
    "def evaluate_performance(y_true, y_pred, classes=['A', 'B', 'C', 'D'], title=\"Model\"):\n",
    "    \"\"\"\n",
    "    Evaluates the performance of a classification model using multiple metrics\n",
    "    and visualizes the confusion matrix.\n",
    "    \"\"\"\n",
    "    # Compute metrics\n",
    "    f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    bal_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"Performance Metrics for {title}:\")\n",
    "    print(f\"F1 Score (Weighted): {f1_weighted:.3f}\")\n",
    "    print(f\"F1 Score (Macro): {f1_macro:.3f}\")\n",
    "    print(f\"MCC: {mcc:.3f}\")\n",
    "    print(f\"Balanced Accuracy: {bal_accuracy:.3f}\") # kick?\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=classes,zero_division=0))\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T21:33:12.291973Z",
     "start_time": "2024-11-22T21:33:11.397966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Data:\n",
      "       Hour     L1     L2     L3        W1         W2\n",
      "16800     0  10.41  59.00  76.38  0.000000  15.036853\n",
      "16801     1   8.33  54.34  49.13  0.000000   1.800000\n",
      "16802     2   8.20  51.96  51.60  0.000000   1.760000\n",
      "16803     3   6.38  47.28  52.23  3.688951   0.000000\n",
      "16804     4   7.37  38.13  51.71  0.000000   0.000000\n"
     ]
    }
   ],
   "source": [
    "# Data Preparation for Multi-Class Classification\n",
    "lagged_features, multiclass_targets = prepare_data(features, targets, lags=3)\n",
    "# Train-Test Split\n",
    "X_train_MC, X_temp_MC, y_train_MC, y_temp_MC = train_test_split(lagged_features, multiclass_targets['category'], test_size=0.3, random_state=42, shuffle=False)\n",
    "X_test_MC, X_val_MC, y_test_MC, y_val_MC = train_test_split(X_temp_MC, y_temp_MC, test_size=0.5, random_state=42, shuffle=False)\n",
    "\n",
    "# Reduce X_test_MC to original columns\n",
    "X_test_MC_reduced = X_test_MC[['wind generation 1', 'wind generation 2', 'load 1', 'load 2', 'load 3', 'Hour_sin', 'Hour_cos']].copy()\n",
    "# Reverse hour cyclic scaling\n",
    "X_test_MC_reduced['Hour'] = (np.arctan2(X_test_MC_reduced['Hour_sin'], X_test_MC_reduced['Hour_cos']) / (2 * np.pi) * 24).round().astype(int) % 24\n",
    "X_test_MC_reduced = X_test_MC_reduced.drop(columns=['Hour_sin', 'Hour_cos'])\n",
    "# Rescale features -> Select only the features to rescale \n",
    "features_to_rescale = ['wind generation 1', 'wind generation 2', 'load 1', 'load 2', 'load 3']\n",
    "non_scaled_features = ['Hour']\n",
    "hour_column = X_test_MC_reduced[non_scaled_features].copy()\n",
    "scaled_features = pd.DataFrame(\n",
    "    scaler.inverse_transform(X_test_MC_reduced[features_to_rescale]),\n",
    "    columns=features_to_rescale,\n",
    "    index=X_test_MC_reduced.index\n",
    ")\n",
    "X_test_MC_reduced = pd.concat([hour_column, scaled_features], axis=1)\n",
    "# Rename & reorder columns\n",
    "X_test_MC_reduced.columns = ['Hour', 'W1', 'W2', 'L1', 'L2', 'L3']\n",
    "X_test_MC_reduced = X_test_MC_reduced[['Hour', 'L1', 'L2', 'L3', 'W1', 'W2']]\n",
    "\n",
    "# Save to csv\n",
    "X_test_MC_reduced.to_csv('Data/X_test_MC.csv', index=False)\n",
    "print(\"\\nTest Data:\")\n",
    "print(X_test_MC_reduced.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T21:33:56.758797Z",
     "start_time": "2024-11-22T21:33:12.294976Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Metrics for Logistic Regression:\n",
      "F1 Score (Weighted): 0.888\n",
      "F1 Score (Macro): 0.678\n",
      "MCC: 0.756\n",
      "Balanced Accuracy: 0.849\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.96      0.89      0.92      2472\n",
      "           B       0.77      0.86      0.81      1008\n",
      "           C       0.06      0.67      0.11         3\n",
      "           D       0.78      0.98      0.87       117\n",
      "\n",
      "    accuracy                           0.88      3600\n",
      "   macro avg       0.64      0.85      0.68      3600\n",
      "weighted avg       0.90      0.88      0.89      3600\n",
      "\n",
      "--------------------------------------------------\n",
      "Performance Metrics for SVM with RBF Kernel:\n",
      "F1 Score (Weighted): 0.921\n",
      "F1 Score (Macro): 0.685\n",
      "MCC: 0.825\n",
      "Balanced Accuracy: 0.688\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.96      0.93      0.95      2472\n",
      "           B       0.84      0.89      0.86      1008\n",
      "           C       0.00      0.00      0.00         3\n",
      "           D       0.93      0.93      0.93       117\n",
      "\n",
      "    accuracy                           0.92      3600\n",
      "   macro avg       0.68      0.69      0.68      3600\n",
      "weighted avg       0.92      0.92      0.92      3600\n",
      "\n",
      "--------------------------------------------------\n",
      "Predicted Values (Logistic Regression):\n",
      "   G1  G2  G3\n",
      "0   1   0   1\n",
      "1   1   0   0\n",
      "2   1   0   1\n",
      "3   1   0   0\n",
      "4   1   0   0\n",
      "Predicted Values (SVM with RBF kernel):\n",
      "   G1  G2  G3\n",
      "0   1   0   1\n",
      "1   1   0   1\n",
      "2   1   0   1\n",
      "3   1   0   0\n",
      "4   1   0   0\n",
      "\n",
      "True Values:\n",
      "   G1  G2  G3\n",
      "0   1   0   1\n",
      "1   1   0   1\n",
      "2   1   0   1\n",
      "3   1   0   0\n",
      "4   1   0   0\n"
     ]
    }
   ],
   "source": [
    "# Train Logistic Regression\n",
    "logreg_model = LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000)\n",
    "logreg_model.fit(X_train_MC, y_train_MC)\n",
    "y_pred_logreg = logreg_model.predict(X_test_MC)\n",
    "# Evaluate\n",
    "evaluate_performance(y_test_MC, y_pred_logreg, title=\"Logistic Regression\")\n",
    "\n",
    "# Train SVM with RBF kernel\n",
    "svm_rbf_model = SVC(kernel='rbf', random_state=42, class_weight='balanced', probability=True, C=10, gamma='scale')\n",
    "svm_rbf_model.fit(X_train_MC, y_train_MC)\n",
    "y_pred_svm_rbf = svm_rbf_model.predict(X_test_MC)\n",
    "# Evaluate\n",
    "evaluate_performance(y_test_MC, y_pred_svm_rbf, title=\"SVM with RBF Kernel\")\n",
    "\n",
    "# Reverse map\n",
    "y_pred_logreg = reverse_map_categories(pd.Series(y_pred_logreg))\n",
    "y_pred_reversed_svm_rbf = reverse_map_categories(pd.Series(y_pred_svm_rbf))\n",
    "y_test_reversed = reverse_map_categories(y_test_MC)\n",
    "\n",
    "# # Save Results to CSV\n",
    "#y_pred_reversed_svm_rbf.to_csv('Data/y_pred_svm.csv', index=False)\n",
    "#y_pred_logreg.to_csv('Data/y_pred_logreg.csv', index=False)\n",
    "#y_test_reversed.to_csv('Data/y_test.csv', index=False)\n",
    "\n",
    "# Print Results\n",
    "print(\"Predicted Values (Logistic Regression):\")\n",
    "print(y_pred_logreg.head())\n",
    "print(\"Predicted Values (SVM with RBF kernel):\")\n",
    "print(y_pred_reversed_svm_rbf.head())\n",
    "print(\"\\nTrue Values:\")\n",
    "print(y_test_reversed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Selection with GridSearch (Hyperparameter Tuning):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T21:39:50.233955Z",
     "start_time": "2024-11-22T21:33:56.760788Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\konst\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Logistic Regression: {'C': 1, 'solver': 'newton-cg'}\n",
      "Best cross-validated MCC: 0.912\n",
      "Performance Metrics for Tuned Logistic Regression:\n",
      "F1 Score (Weighted): 0.911\n",
      "F1 Score (Macro): 0.685\n",
      "MCC: 0.801\n",
      "Balanced Accuracy: 0.679\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.93      0.95      0.94      2472\n",
      "           B       0.86      0.82      0.84      1008\n",
      "           C       0.00      0.00      0.00         3\n",
      "           D       0.97      0.95      0.96       117\n",
      "\n",
      "    accuracy                           0.91      3600\n",
      "   macro avg       0.69      0.68      0.68      3600\n",
      "weighted avg       0.91      0.91      0.91      3600\n",
      "\n",
      "--------------------------------------------------\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Hyperparameter Tuning\n",
    "def tune_log_reg(X_train, y_train):\n",
    "    param_grid = {\n",
    "        'C': [0.01, 0.1, 1, 10],  # Regularization strength\n",
    "        'solver': ['lbfgs', 'newton-cg', 'saga'],  # Optimization algorithm\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        LogisticRegression(random_state=42, max_iter=1000, multi_class='multinomial', penalty='l2'),\n",
    "        param_grid,\n",
    "        scoring='f1_weighted',\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(f\"Best parameters for Logistic Regression: {grid_search.best_params_}\")\n",
    "    print(f\"Best cross-validated MCC: {grid_search.best_score_:.3f}\")\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "\n",
    "# SVM Hyperparameter Tuning\n",
    "def tune_svm(X_train, y_train):\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10],  # Regularization strength\n",
    "        'kernel': ['linear', 'poly', 'rbf'],  # Kernels to evaluate\n",
    "        'degree': [2, 3],  # Degree of polynomial kernel (ignored for others)\n",
    "        'gamma': ['scale', 'auto'],  # Kernel coefficient\n",
    "    }\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        SVC(random_state=42),\n",
    "        param_grid,\n",
    "        scoring='f1_weighted',\n",
    "        cv=5,  # 5-fold cross-validation\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(f\"Best parameters for SVM: {grid_search.best_params_}\")\n",
    "    print(f\"Best cross-validated MCC: {grid_search.best_score_:.3f}\")\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "# Fine-tune Logistic Regression\n",
    "best_logistic_model = tune_log_reg(X_train_MC, y_train_MC)\n",
    "\n",
    "# Evaluate Logistic Regression with Best Parameters\n",
    "y_pred_logreg_tune = best_logistic_model.predict(X_test_MC)\n",
    "evaluate_performance(y_test_MC, y_pred_logreg_tune, title=\"Tuned Logistic Regression\")\n",
    "\n",
    "# Fine-tune SVM\n",
    "best_svm_model = tune_svm(X_train_MC, y_train_MC)\n",
    "\n",
    "# Evaluate SVM with Best Parameters\n",
    "y_pred_svm_tune = best_svm_model.predict(X_test_MC)\n",
    "evaluate_performance(y_test_MC, y_pred_svm_tune, title=\"Tuned SVM\")\n",
    "\n",
    "# Reverse map\n",
    "y_pred_logreg_tune = reverse_map_categories(pd.Series(y_pred_logreg_tune))\n",
    "y_pred_svm_tune = reverse_map_categories(pd.Series(y_pred_svm_tune))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T21:39:50.311230Z",
     "start_time": "2024-11-22T21:39:50.237490Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Values (Tuned Logistic Regression):\n",
      "   Hour  G1  G2  G3\n",
      "0     0   1   0   1\n",
      "1     1   1   0   1\n",
      "2     2   1   0   1\n",
      "3     3   1   0   0\n",
      "4     4   1   0   0\n",
      "Predicted Values (Tuned SVM):\n",
      "   Hour  G1  G2  G3\n",
      "0     0   1   0   1\n",
      "1     1   1   0   1\n",
      "2     2   1   0   1\n",
      "3     3   1   0   0\n",
      "4     4   1   0   0\n",
      "\n",
      "True Values:\n",
      "      Hour  G1  G2  G3\n",
      "3595    19   1   1   1\n",
      "3596    20   1   1   1\n",
      "3597    21   1   1   1\n",
      "3598    22   1   1   1\n",
      "3599    23   1   1   1\n"
     ]
    }
   ],
   "source": [
    "num_rows = len(y_pred_logreg_tune)\n",
    "hours = list(range(24)) * (num_rows // 24) + list(range(num_rows % 24))  # Create repeating hours (0-23)\n",
    "# Add the 'Hour' column to y_pred_logreg_tune, y_pred_svm_tune, and y_test_reversed\n",
    "y_pred_logreg_tune['Hour'] = hours[:num_rows]\n",
    "y_pred_svm_tune['Hour'] = hours[:num_rows]\n",
    "y_test_reversed['Hour'] = hours[:num_rows]\n",
    "\n",
    "\n",
    "# Ensure 'Hour' is the first column\n",
    "y_pred_logreg_tune = y_pred_logreg_tune[['Hour', 'G1', 'G2', 'G3']]\n",
    "y_pred_svm_tune = y_pred_svm_tune[['Hour', 'G1', 'G2', 'G3']]\n",
    "y_test_reversed = y_test_reversed[['Hour', 'G1', 'G2', 'G3']]\n",
    "\n",
    "# Save Results to CSV\n",
    "y_pred_logreg_tune.to_csv('Data/y_pred_logreg_tune.csv', index=False)\n",
    "y_pred_svm_tune.to_csv('Data/y_pred_svm_tune.csv', index=False)\n",
    "y_test_reversed.to_csv('Data/y_test.csv', index=False)\n",
    "\n",
    "# Print results\n",
    "print(\"Predicted Values (Tuned Logistic Regression):\")\n",
    "print(y_pred_logreg_tune.head())\n",
    "print(\"Predicted Values (Tuned SVM):\")\n",
    "print(y_pred_svm_tune.head())\n",
    "print(\"\\nTrue Values:\")\n",
    "print(y_test_reversed.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T21:46:40.766531Z",
     "start_time": "2024-11-22T21:39:50.315229Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing Unit: G2 ---\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\konst\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1237: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, binary problems will be fit as proper binary  logistic regression models (as if multi_class='ovr' were set). Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Logistic Regression: {'C': 0.01, 'solver': 'lbfgs'}\n",
      "Best cross-validated MCC: 0.915\n",
      "--- Logistic Regression Performance for G2 ---\n",
      "Performance Metrics for Tuned Logistic Regression - G2:\n",
      "F1 Score (Weighted): 0.914\n",
      "F1 Score (Macro): 0.899\n",
      "MCC: 0.798\n",
      "Balanced Accuracy: 0.893\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.84      0.86      1125\n",
      "         1.0       0.93      0.95      0.94      2475\n",
      "\n",
      "    accuracy                           0.91      3600\n",
      "   macro avg       0.91      0.89      0.90      3600\n",
      "weighted avg       0.91      0.91      0.91      3600\n",
      "\n",
      "--------------------------------------------------\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Best parameters for SVM: {'C': 10, 'degree': 2, 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "Best cross-validated MCC: 0.936\n",
      "--- SVM Performance for G2 ---\n",
      "Performance Metrics for Tuned SVM - G2:\n",
      "F1 Score (Weighted): 0.932\n",
      "F1 Score (Macro): 0.921\n",
      "MCC: 0.842\n",
      "Balanced Accuracy: 0.916\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.87      0.89      1125\n",
      "         1.0       0.94      0.96      0.95      2475\n",
      "\n",
      "    accuracy                           0.93      3600\n",
      "   macro avg       0.93      0.92      0.92      3600\n",
      "weighted avg       0.93      0.93      0.93      3600\n",
      "\n",
      "--------------------------------------------------\n",
      "Finished processing unit: G2\n",
      "\n",
      "--- Processing Unit: G3 ---\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\konst\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1237: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, binary problems will be fit as proper binary  logistic regression models (as if multi_class='ovr' were set). Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Logistic Regression: {'C': 1, 'solver': 'newton-cg'}\n",
      "Best cross-validated MCC: 0.995\n",
      "--- Logistic Regression Performance for G3 ---\n",
      "Performance Metrics for Tuned Logistic Regression - G3:\n",
      "F1 Score (Weighted): 0.996\n",
      "F1 Score (Macro): 0.972\n",
      "MCC: 0.944\n",
      "Balanced Accuracy: 0.970\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.94      0.95       120\n",
      "         1.0       1.00      1.00      1.00      3480\n",
      "\n",
      "    accuracy                           1.00      3600\n",
      "   macro avg       0.97      0.97      0.97      3600\n",
      "weighted avg       1.00      1.00      1.00      3600\n",
      "\n",
      "--------------------------------------------------\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Best parameters for SVM: {'C': 1, 'degree': 2, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "Best cross-validated MCC: 0.996\n",
      "--- SVM Performance for G3 ---\n",
      "Performance Metrics for Tuned SVM - G3:\n",
      "F1 Score (Weighted): 0.997\n",
      "F1 Score (Macro): 0.976\n",
      "MCC: 0.952\n",
      "Balanced Accuracy: 0.962\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.93      0.95       120\n",
      "         1.0       1.00      1.00      1.00      3480\n",
      "\n",
      "    accuracy                           1.00      3600\n",
      "   macro avg       0.99      0.96      0.98      3600\n",
      "weighted avg       1.00      1.00      1.00      3600\n",
      "\n",
      "--------------------------------------------------\n",
      "Finished processing unit: G3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define function for binary classification loop\n",
    "def binary_classification_loop(features_lagged, targets, units_to_classify):\n",
    "    results = {}\n",
    "    for unit in units_to_classify:\n",
    "        print(f\"--- Processing Unit: {unit} ---\")\n",
    "\n",
    "        # Use the specific unit as the target\n",
    "        target = targets[[unit]]\n",
    "\n",
    "        # Train-Test-Validation Split\n",
    "        X_train_L, X_temp_L, y_train_L, y_temp_L = train_test_split(\n",
    "            features_lagged, target, test_size=0.3, random_state=42, shuffle=False\n",
    "        )\n",
    "        X_test_L, X_val_L, y_test_L, y_val_L = train_test_split(\n",
    "            X_temp_L, y_temp_L, test_size=0.5, random_state=42, shuffle=False\n",
    "        )\n",
    "\n",
    "        # Train and Evaluate Logistic Regression\n",
    "        best_logistic_model = tune_log_reg(X_train_L, y_train_L.values.ravel())\n",
    "        y_pred_logreg = best_logistic_model.predict(X_test_L)\n",
    "        print(f\"--- Logistic Regression Performance for {unit} ---\")\n",
    "        evaluate_performance(y_test_L, y_pred_logreg, classes=None, title=f\"Tuned Logistic Regression - {unit}\")\n",
    "\n",
    "        # Train and Evaluate SVM\n",
    "        best_svm_model = tune_svm(X_train_L, y_train_L.values.ravel())\n",
    "        y_pred_svm = best_svm_model.predict(X_test_L)\n",
    "        print(f\"--- SVM Performance for {unit} ---\")\n",
    "        evaluate_performance(y_test_L, y_pred_svm, classes=None, title=f\"Tuned SVM - {unit}\")\n",
    "\n",
    "        # Save Results\n",
    "        results[unit] = {\n",
    "            'logistic': {\n",
    "                'predictions': y_pred_logreg,\n",
    "                'best_model': best_logistic_model,\n",
    "            },\n",
    "            'svm': {\n",
    "                'predictions': y_pred_svm,\n",
    "                'best_model': best_svm_model,\n",
    "            }\n",
    "        }\n",
    "        print(f\"Finished processing unit: {unit}\\n\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Adjust targets\n",
    "lagged_targets = targets.iloc[3:].reset_index(drop=True)  \n",
    "\n",
    "# Call the function\n",
    "binary_results = binary_classification_loop(lagged_features, lagged_targets, units_to_classify)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T21:46:40.872100Z",
     "start_time": "2024-11-22T21:46:40.773530Z"
    }
   },
   "outputs": [],
   "source": [
    "#Initialize predictions with G1 column (filled with 1s for all rows based on the length of predictions)\n",
    "num_rows = len(next(iter(binary_results.values()))['logistic']['predictions'])  # Get the number of rows from any prediction\n",
    "logistic_predictions = pd.DataFrame({'G1': [1] * num_rows})  # G1 is always 1\n",
    "svm_predictions = pd.DataFrame({'G1': [1] * num_rows})  # G1 is always 1\n",
    "# Add the 'Hour' column to the logistic and SVM predictions\n",
    "\n",
    "# Add G2 and G3 predictions\n",
    "logistic_predictions['G2'] = binary_results['G2']['logistic']['predictions']\n",
    "logistic_predictions['G3'] = binary_results['G3']['logistic']['predictions']\n",
    "\n",
    "svm_predictions['G2'] = binary_results['G2']['svm']['predictions']\n",
    "svm_predictions['G3'] = binary_results['G3']['svm']['predictions']\n",
    "\n",
    "# Add the 'Hour' column to the logistic and SVM predictions\n",
    "logistic_predictions['Hour'] = hours[:num_rows]\n",
    "svm_predictions['Hour'] = hours[:num_rows]\n",
    "\n",
    "# Reorder columns to place 'Hour' first\n",
    "logistic_predictions = logistic_predictions[['Hour', 'G1', 'G2', 'G3']]\n",
    "svm_predictions = svm_predictions[['Hour', 'G1', 'G2', 'G3']]\n",
    "\n",
    "# Save consolidated predictions to CSV\n",
    "logistic_predictions.to_csv('Data/logistic_predictions.csv', index=False)\n",
    "svm_predictions.to_csv('Data/svm_predictions.csv', index=False)\n",
    "# Optional: Save results for each unit\n",
    "#for unit, result in binary_results.items():\n",
    "   # pd.DataFrame(result['logistic']['predictions'], columns=[f\"{unit}_logistic\"]).to_csv(f'Data/{unit}_logistic_predictions.csv', index=False)\n",
    "    #pd.DataFrame(result['svm']['predictions'], columns=[f\"{unit}_svm\"]).to_csv(f'Data/{unit}_svm_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the output of the classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Insert the csv file path that shall be tested in y_pred \n",
    "# import the csv files X_val, y_val and y_pred\n",
    "X_val = pd.read_csv('../Data/X_test_MC.csv', delimiter=',')\n",
    "#y_val = pd.read_csv('../Data/y_val.csv', delimiter=',')\n",
    "y_pred = pd.read_csv('../Data/y_pred_logreg_tune.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the data folder\n",
    "bus = pd.read_csv('../Data/B (power transfer factor of each bus to each line).csv', delimiter=';')\n",
    "max_prod = pd.read_csv('../Data/Maximum production of generating units.csv', delimiter=';')\n",
    "min_prod = pd.read_csv('../Data/Minimum production of generating units.csv', delimiter=';')\n",
    "min_down_time = pd.read_csv('../Data/Minimum down time of generating units.csv', delimiter=';')\n",
    "min_up_time = pd.read_csv('../Data/Minimum up time of generating units.csv', delimiter=';')\n",
    "prod_cost = pd.read_csv('../Data/Production cost of generating units.csv', delimiter=';')\n",
    "ramp_rate = pd.read_csv('../Data/Ramping rate of generating units.csv', delimiter=';')\n",
    "start_up_cost = pd.read_csv('../Data/Start-up cost of generating units.csv', delimiter=';')\n",
    "transmission_cap = pd.read_csv('../Data/Transmission capacity of lines.csv', delimiter=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nodes = ['Node 1', 'Node 2', 'Node 3', 'Node 4', 'Node 5', 'Node 6']\n",
    "Generator = ['G1', 'G2', 'G3']\n",
    "Generator_node = {'Node 1': 'G1', 'Node 2': 'G2', 'Node 6': 'G3'}\n",
    "Load = ['L1', 'L2', 'L3']\n",
    "Load_node = {'Node 4': 'L1', 'Node 5': 'L2', 'Node 6': 'L3'}\n",
    "Wind = ['W1', 'W2']\n",
    "Wind_node = {'Node 4': 'W1', 'Node 5': 'W2'}\n",
    "Transmission = ['Line 1', 'Line 2', 'Line 3', 'Line 4', 'Line 5', 'Line 6','Line 7']\n",
    "Transmission_node = {'Line 1': ['Node 1', 'Node 2'], 'Line 2': ['Node 2', 'Node 3'], 'Line 3': ['Node 3', 'Node 6'], 'Line 4': ['Node 5', 'Node 6'], 'Line 5': ['Node 4', 'Node 5'], 'Line 6': ['Node 2', 'Node 4'],'Line 6': ['Node 1', 'Node 4']}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrix with the nodes as columns and the generators, loads and winds as rows, with 1 if connected to the node\n",
    "Gen_n = np.zeros((len(Generator), len(Nodes)))\n",
    "Load_n = np.zeros((len(Load), len(Nodes)))\n",
    "Wind_n = np.zeros((len(Wind), len(Nodes)))\n",
    "Transmission_n = np.zeros((len(Transmission), len(Nodes)))\n",
    "\n",
    "# Populate the matrix\n",
    "for i, g in enumerate(Generator):  # Iterate over generators\n",
    "    for j, node in enumerate(Nodes):  # Iterate over nodes\n",
    "        if Generator_node.get(node) == g:  # Check if generator is connected to the node\n",
    "            Gen_n[i, j] = 1\n",
    "\n",
    "for i, l in enumerate(Load):  # Iterate over loads\n",
    "    for j, node in enumerate(Nodes):  # Iterate over nodes\n",
    "        if Load_node.get(node) == l:  # Check if load is connected to the node\n",
    "            Load_n[i, j] = 1\n",
    "\n",
    "for i, w in enumerate(Wind):  # Iterate over winds\n",
    "    for j, node in enumerate(Nodes):  # Iterate over nodes\n",
    "        if Wind_node.get(node) == w:  # Check if wind is connected to the node\n",
    "            Wind_n[i, j] = 1\n",
    "\n",
    "for i, t in enumerate(Transmission):  # Iterate over transmission lines\n",
    "    connected_nodes = Transmission_node.get(t, [])  # Get nodes connected by the transmission line\n",
    "    for node in connected_nodes:  # For each node connected by the transmission line\n",
    "        if node in Nodes:  # Ensure the node is valid (exists in Nodes list)\n",
    "            j = Nodes.index(node)  # Get the column index for the node in Transmission_n\n",
    "            Transmission_n[i, j] = 1  # Set the corresponding element to 1\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input data class\n",
    "class InputData:\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        wind_forecast: pd.DataFrame, \n",
    "        bus: pd.DataFrame,\n",
    "        load: pd.DataFrame,\n",
    "        max_prod: pd.DataFrame,\n",
    "        min_prod: pd.DataFrame,\n",
    "        min_down_time: pd.DataFrame,\n",
    "        min_up_time: pd.DataFrame,\n",
    "        prod_cost: pd.DataFrame,\n",
    "        ramp_rate: pd.DataFrame,\n",
    "        start_up_cost: pd.DataFrame,\n",
    "        transmission_cap: pd.DataFrame\n",
    "    ):\n",
    "        self.time = range(len(wind_forecast))  #maybe define it with lenght of wind_production\n",
    "        self.wind_forecast = wind_forecast\n",
    "        self.bus = bus\n",
    "        self.load = load\n",
    "        self.max_prod = max_prod\n",
    "        self.min_prod = min_prod\n",
    "        self.min_down_time = min_down_time\n",
    "        self.min_up_time = min_up_time\n",
    "        self.prod_cost = prod_cost\n",
    "        self.ramp_rate = ramp_rate\n",
    "        self.start_up_cost = start_up_cost\n",
    "        self.transmission_cap = transmission_cap\n",
    "        self.M = 1000000  # Penalty for having flexible demand\n",
    "        self.Gen_n = Gen_n  # Matrix mapping generators to nodes\n",
    "        self.Load_n = Load_n # Matrix mapping loads to nodes\n",
    "        self.Wind_n = Wind_n # Matrix mapping wind to nodes\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expando(object):\n",
    "    '''\n",
    "        A small class which can have attributes set\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimization model class\n",
    "\n",
    "class EconomicDispatch_Test():\n",
    "        \n",
    "        def __init__(self, input_data: InputData, y_pred: pd.DataFrame):\n",
    "            self.data = input_data \n",
    "            self.y_pred = y_pred.T\n",
    "            self.variables = Expando()\n",
    "            self.constraints = Expando() \n",
    "            self.results = Expando() \n",
    "            self._build_model() \n",
    "            \n",
    "        def _build_variables(self):\n",
    "            # one variable for each generator for each time of the day\n",
    "            self.variables.prod_gen = {\n",
    "                 (i, t): self.model.addVar(lb=0, ub=self.data.max_prod.iloc[i-1, 0], \n",
    "                                           name='generation_G{}_{}'.format(i, t)) \n",
    "                                           for i in range(1, len(self.data.max_prod)+1) \n",
    "                                           for t in self.data.time}\n",
    "            \n",
    "            # one variable for each wind generator for each time of the day\n",
    "            self.variables.prod_wind = {\n",
    "                 (i, t): self.model.addVar(lb=0, ub=self.data.wind_forecast.iloc[t, i], \n",
    "                                            name='wind_generation_W{}_{}'.format(i, t)) \n",
    "                                            for i in range(1, len(self.data.wind_forecast.iloc[0, :])) \n",
    "                                            for t in self.data.time}\n",
    "            \n",
    "            # one variable for each start-up cost for each generator\n",
    "            self.variables.start_up_cost = {\n",
    "                 (i, t): self.model.addVar(lb=0, \n",
    "                                            name='start_up_cost_G{}_{}'.format(i, t)) \n",
    "                                            for i in range(1, len(self.data.max_prod)+1) \n",
    "                                            for t in self.data.time}\n",
    "            \n",
    "            # add two slack variables to always make the model feasible, allowing the demand to be flexible\n",
    "            self.variables.epsilon = {\n",
    "                 (n, t): self.model.addVar(lb=0, \n",
    "                                           name='epsilon_Bus{}_{}'.format(n, t)) \n",
    "                                           for n in range(1, len(self.data.bus.iloc[0,:])+1) \n",
    "                                           for t in self.data.time}\n",
    "            self.variables.delta = {\n",
    "                 (n, t): self.model.addVar(lb=0, \n",
    "                                           name='delta_Bus{}_{}'.format(n, t))\n",
    "                                           for n in range(1, len(self.data.bus.iloc[0,:])+1)\n",
    "                                           for t in self.data.time}\n",
    "            \n",
    "            # add two slack variables to always make the model feasible, relaxing the min_up_time and min_down_time constraints\n",
    "            self.variables.alpha = {\n",
    "                    (i, t, to): self.model.addVar(lb=0,\n",
    "                                            name='alpha_G{}_{}_{}'.format(i, t, to))\n",
    "                                            for i in range(1, len(self.data.max_prod)+1)\n",
    "                                            for t in self.data.time\n",
    "                                            for to in range(t, min(t + self.data.min_up_time.iloc[i-1, 0], len(self.data.time))) if t > 0}\n",
    "            self.variables.beta = {\n",
    "                    (i, t, to): self.model.addVar(lb=0,\n",
    "                                            name='beta_G{}_{}_{}'.format(i, t, to))\n",
    "                                            for i in range(1, len(self.data.max_prod)+1)\n",
    "                                            for t in self.data.time\n",
    "                                            for to in range(t, min(t + self.data.min_down_time.iloc[i-1, 0], len(self.data.time))) if t > 0}\n",
    "\n",
    "            \n",
    "            \n",
    "        def _build_constraints(self):\n",
    "            # Minimum capacity of the generator\n",
    "            self.constraints.min_capacity = {\n",
    "                (i, t): self.model.addConstr(\n",
    "                    self.variables.prod_gen[i, t] >= self.data.min_prod.iloc[i-1, 0] * self.y_pred.iloc[i, t]\n",
    "                ) for i in range(1, len(self.data.max_prod)+1) for t in self.data.time}\n",
    "            # Maximum capacity of the generator\n",
    "            self.constraints.max_capacity = {\n",
    "                (i, t): self.model.addConstr(\n",
    "                    self.variables.prod_gen[i, t] <= self.data.max_prod.iloc[i-1, 0] * self.y_pred.iloc[i, t]\n",
    "                ) for i in range(1, len(self.data.max_prod)+1) for t in self.data.time}\n",
    "\n",
    "            # Power balance constraint\n",
    "            self.constraints.power_balance = {\n",
    "                t: self.model.addConstr(\n",
    "                    gp.quicksum(self.variables.prod_gen[i, t] for i in range(1, len(self.data.max_prod) + 1)) +\n",
    "                    gp.quicksum(self.variables.prod_wind[i, t] for i in range(1, len(self.data.wind_forecast.iloc[0, :]))) == \n",
    "                    gp.quicksum(self.data.load.iloc[t, i] * Load_n[i-1, n-1] for i in range(1, len(self.data.load.iloc[0, :]))for n in range(1, len(self.data.bus.iloc[0, :]) + 1))\n",
    "                    + gp.quicksum(self.variables.epsilon[n, t] - self.variables.delta[n, t] for n in range(1, len(self.data.bus.iloc[0, :]) + 1))\n",
    "                ) for t in self.data.time}\n",
    "        \n",
    "            # Transmission capacity constraint up\n",
    "            self.constraints.transmission_capacity_up = {\n",
    "                    (l, t): self.model.addConstr(\n",
    "                        gp.quicksum(\n",
    "                            self.data.bus.iloc[l-1, n-1] * Transmission_n[l-1, n-1] * (\n",
    "                                self.variables.prod_gen[g, t] * Gen_n[g-1, n-1] +\n",
    "                                self.variables.prod_wind[w, t] * Wind_n[w-1, n-1] -\n",
    "                                self.data.load.iloc[t, i] * Load_n[i-1, n-1] -\n",
    "                                self.variables.epsilon[n, t] +\n",
    "                                self.variables.delta[n, t]\n",
    "                            )\n",
    "                            for n in range(1, len(self.data.bus.iloc[0, :]) + 1)\n",
    "                            for i in range(1, len(self.data.load.iloc[0, :]))\n",
    "                            for g in range(1, len(self.data.max_prod) + 1)\n",
    "                            for w in range(1, len(self.data.wind_forecast.iloc[0, :]))\n",
    "                        ) <= self.data.transmission_cap.iloc[l-1, 0],\n",
    "                        name=\"transmission_capacity_up_L{}_T{}\".format(l, t)\n",
    "                    ) for l in range(1, len(self.data.transmission_cap) + 1)\n",
    "                    for t in self.data.time\n",
    "                }\n",
    "\n",
    "            #Transmission capacity constraint down\n",
    "            self.constraints.transmission_capacity_down = {\n",
    "                    (l, t): self.model.addConstr(\n",
    "                        gp.quicksum(\n",
    "                            self.data.bus.iloc[l-1, n-1] * Transmission_n[l-1, n-1] * (\n",
    "                                self.variables.prod_gen[g, t] * Gen_n[g-1, n-1] +\n",
    "                                self.variables.prod_wind[w, t] * Wind_n[w-1, n-1] -\n",
    "                                self.data.load.iloc[t, i] * Load_n[i-1, n-1] -\n",
    "                                self.variables.epsilon[n, t] +\n",
    "                                self.variables.delta[n, t]\n",
    "                            )\n",
    "                            for n in range(1, len(self.data.bus.iloc[0, :]) + 1)\n",
    "                            for i in range(1, len(self.data.load.iloc[0, :]))\n",
    "                            for g in range(1, len(self.data.max_prod) + 1)\n",
    "                            for w in range(1, len(self.data.wind_forecast.iloc[0, :]))\n",
    "                        ) >= -self.data.transmission_cap.iloc[l-1, 0],\n",
    "                        name=\"transmission_capacity_down_L{}_T{}\".format(l, t)\n",
    "                    ) for l in range(1, len(self.data.transmission_cap) + 1)\n",
    "                    for t in self.data.time\n",
    "                }\n",
    "\n",
    "            #Start-up costs constraint\n",
    "            self.constraints.start_up_cost = {\n",
    "                (i, t): self.model.addConstr(\n",
    "                    self.variables.start_up_cost[i, t] >= self.data.start_up_cost.iloc[i-1, 0] * (self.y_pred.iloc[i, t] - self.y_pred.iloc[i, t-1])\n",
    "                ) for i in range(1, len(self.data.max_prod)+1) for t in self.data.time if t > 0}\n",
    "            self.constraints.start_up_cost_0 = {\n",
    "                i: self.model.addConstr(\n",
    "                    self.variables.start_up_cost[i, 0] >= self.data.start_up_cost.iloc[i-1, 0] * self.y_pred.iloc[i, 0]\n",
    "                ) for i in range(1, len(self.data.max_prod)+1)}\n",
    "            \n",
    "            # Ramping constraint\n",
    "            self.constraints.ramping_up = {\n",
    "                (i, t): self.model.addConstr(\n",
    "                    self.variables.prod_gen[i, t] - self.variables.prod_gen[i, t-1] <= self.data.ramp_rate.iloc[i-1, 0]\n",
    "                ) for i in range(1, len(self.data.max_prod)+1) for t in self.data.time if t > 0}\n",
    "            self.constraints.ramping_down = {\n",
    "                (i, t): self.model.addConstr(\n",
    "                    self.variables.prod_gen[i, t-1] - self.variables.prod_gen[i, t] <= self.data.ramp_rate.iloc[i-1, 0]\n",
    "                ) for i in range(1, len(self.data.max_prod)+1) for t in self.data.time if t > 0}\n",
    "            \n",
    "            # Minimum up time constraint\n",
    "            self.constraints.min_up_time = {\n",
    "                (i, t, to): self.model.addConstr(\n",
    "                    -self.y_pred.iloc[i, t - 1] + self.y_pred.iloc[i, t] - self.y_pred.iloc[i, to] - self.variables.alpha[i, t, to] <= 0\n",
    "                ) for i in range(1, len(self.data.max_prod)+1) \n",
    "                for t in self.data.time \n",
    "                for to in range(t, min(t + self.data.min_up_time.iloc[i-1, 0], len(self.data.time))) if t > 0}\n",
    "            \n",
    "            # Minimum down time constraint\n",
    "            self.constraints.min_down_time = {\n",
    "                (i, t, to): self.model.addConstr(\n",
    "                    self.y_pred.iloc[i, t - 1] - self.y_pred.iloc[i, t] + self.y_pred.iloc[i, to] - self.variables.beta[i, t, to] <= 1\n",
    "                ) for i in range(1, len(self.data.max_prod)+1) \n",
    "                for t in self.data.time \n",
    "                for to in range(t, min(t + self.data.min_down_time.iloc[i-1, 0], len(self.data.time))) if t > 0}\n",
    "            \n",
    "\n",
    "\n",
    "        def _build_objective(self):\n",
    "            # Objective function\n",
    "            self.model.setObjective(\n",
    "                gp.quicksum(self.data.prod_cost.iloc[i-1, 0]*self.variables.prod_gen[i, t] for i in range(1, len(self.data.max_prod)+1) for t in self.data.time) +\n",
    "                gp.quicksum(self.variables.start_up_cost[i, t] for i in range(1, len(self.data.max_prod)+1) for t in self.data.time) +\n",
    "                self.data.M * (gp.quicksum(self.variables.epsilon[n, t] + self.variables.delta[n, t] for n in range(1, len(self.data.bus.iloc[0,:])+1) for t in self.data.time)) +\n",
    "                self.data.M * (gp.quicksum(self.variables.alpha[i, t, to] for i in range(1, len(self.data.max_prod)+1) for t in self.data.time for to in range(t, min(t + self.data.min_up_time.iloc[i-1, 0], len(self.data.time))) if t > 0)) +\n",
    "                self.data.M * (gp.quicksum(self.variables.beta[i, t, to] for i in range(1, len(self.data.max_prod)+1) for t in self.data.time for to in range(t, min(t + self.data.min_down_time.iloc[i-1, 0], len(self.data.time))) if t > 0))\n",
    "            )\n",
    "\n",
    "        def _build_model(self):\n",
    "            self.model = gp.Model('EconomicDispatch')\n",
    "            self._build_variables()\n",
    "            self._build_constraints()\n",
    "            self._build_objective()\n",
    "            self.model.update()\n",
    "\n",
    "        def optimize(self):\n",
    "            self.model.optimize()\n",
    "            self._extract_results()\n",
    "\n",
    "        def _extract_results(self):\n",
    "            self.results.production = pd.DataFrame({\n",
    "                \n",
    "                'hour': [self.data.wind_forecast.iloc[t, 0] for t in self.data.time],\n",
    "                #'time': [t for t in self.data.time],\n",
    "                #'status G1': [self.y_pred.iloc[1, t] for t in self.data.time],\n",
    "                #'status G2': [self.y_pred.iloc[2, t] for t in self.data.time],\n",
    "                #'status G3': [self.y_pred.iloc[3, t] for t in self.data.time],\n",
    "                #'start_up_cost 1': [self.variables.start_up_cost[1, t].x for t in self.data.time],\n",
    "                #'start_up_cost 2': [self.variables.start_up_cost[2, t].x for t in self.data.time],\n",
    "                #'start_up_cost 3': [self.variables.start_up_cost[3, t].x for t in self.data.time],\n",
    "                #'generation 1': [self.variables.prod_gen[1, t].x for t in self.data.time],\n",
    "                #'generation 2': [self.variables.prod_gen[2, t].x for t in self.data.time],\n",
    "                #'generation 3': [self.variables.prod_gen[3, t].x for t in self.data.time],\n",
    "                #'wind generation 1': [self.variables.prod_wind[1, t].x for t in self.data.time],\n",
    "                #'wind generation 2': [self.variables.prod_wind[2, t].x for t in self.data.time],\n",
    "                #'load 1': [self.data.load.iloc[t, 1] for t in self.data.time],\n",
    "                #'load 2': [self.data.load.iloc[t, 2] for t in self.data.time],\n",
    "                #'load 3': [self.data.load.iloc[t, 3] for t in self.data.time],\n",
    "                #'epsilon 1': [self.variables.epsilon[1, t].x for t in self.data.time],\n",
    "                #'delta 1': [self.variables.delta[1, t].x for t in self.data.time],\n",
    "                #'epsilon 2': [self.variables.epsilon[2, t].x for t in self.data.time],\n",
    "                #'delta 2': [self.variables.delta[2, t].x for t in self.data.time],\n",
    "                #'epsilon 3': [self.variables.epsilon[3, t].x for t in self.data.time],\n",
    "                #'delta 3': [self.variables.delta[3, t].x for t in self.data.time],\n",
    "                #'epsilon 4': [self.variables.epsilon[4, t].x for t in self.data.time],\n",
    "                #'delta 4': [self.variables.delta[4, t].x for t in self.data.time],\n",
    "                #'epsilon 5': [self.variables.epsilon[5, t].x for t in self.data.time],\n",
    "                #'delta 5': [self.variables.delta[5, t].x for t in self.data.time],\n",
    "                #'epsilon 6': [self.variables.epsilon[6, t].x for t in self.data.time],\n",
    "                #'delta 6': [self.variables.delta[6, t].x for t in self.data.time]\n",
    "            })\n",
    "\n",
    "            # Add columns for each transmission line's binding status at each time\n",
    "            for l in range(1, len(self.data.transmission_cap) + 1):\n",
    "                up_binding = []\n",
    "                down_binding = []\n",
    "                \n",
    "                for t in self.data.time:\n",
    "                    up_constraint = self.constraints.transmission_capacity_up[l, t]\n",
    "                    down_constraint = self.constraints.transmission_capacity_down[l, t]\n",
    "\n",
    "                    # Append binding status (True if binding, based on slack value)\n",
    "                    up_binding.append(abs(up_constraint.slack) < 1e-6)\n",
    "                    down_binding.append(abs(down_constraint.slack) < 1e-6)\n",
    "\n",
    "                # Add the binding status as new columns in the main production DataFrame\n",
    "                self.results.production[f'transmission_up_binding_L{l}'] = up_binding\n",
    "                self.results.production[f'transmission_down_binding_L{l}'] = down_binding\n",
    "\n",
    "            # Add the alpha and beta variables to the results\n",
    "            self.results.alpha1 = [gp.quicksum(self.variables.alpha[1, t, to].x  for to in range(t, min(t + self.data.min_up_time.iloc[0, 0], len(self.data.time))) if t > 0) for t in self.data.time]\n",
    "            self.results.alpha2 = [gp.quicksum(self.variables.alpha[2, t, to].x  for to in range(t, min(t + self.data.min_up_time.iloc[1, 0], len(self.data.time))) if t > 0) for t in self.data.time]\n",
    "            self.results.alpha3 = [gp.quicksum(self.variables.alpha[3, t, to].x  for to in range(t, min(t + self.data.min_up_time.iloc[2, 0], len(self.data.time))) if t > 0) for t in self.data.time]\n",
    "            self.results.beta1 = [gp.quicksum(self.variables.beta[1, t, to].x  for to in range(t, min(t + self.data.min_down_time.iloc[0, 0], len(self.data.time))) if t > 0) for t in self.data.time]\n",
    "            self.results.beta2 = [gp.quicksum(self.variables.beta[2, t, to].x  for to in range(t, min(t + self.data.min_down_time.iloc[1, 0], len(self.data.time))) if t > 0) for t in self.data.time]\n",
    "            self.results.beta3 = [gp.quicksum(self.variables.beta[3, t, to].x  for to in range(t, min(t + self.data.min_down_time.iloc[2, 0], len(self.data.time))) if t > 0) for t in self.data.time]\n",
    "            \n",
    "\n",
    "                                  \n",
    "            self.results.objective = self.model.objVal\n",
    "            \n",
    "        def _print_model(self):\n",
    "            self.model.write('EconomicDispatch.lp')        \n",
    "            \n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model test\n",
    "wind_forecast_test = X_val.drop(columns=['L1', 'L2', 'L3'])\n",
    "load_test = X_val.drop(columns=['W1', 'W2'])\n",
    "input_data = InputData(wind_forecast_test, bus, load_test, max_prod, min_prod, min_down_time, min_up_time, prod_cost, ramp_rate, start_up_cost, transmission_cap)\n",
    "model_test = EconomicDispatch_Test(input_data, y_pred)\n",
    "#model_test = EconomicDispatch_Test(input_data, y_pred.head(24))\n",
    "model_test.optimize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
