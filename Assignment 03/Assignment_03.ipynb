{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T08:38:17.368480Z",
     "start_time": "2024-12-10T08:38:17.352835Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.dates as mdates\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T08:38:42.705030Z",
     "start_time": "2024-12-10T08:38:42.488865Z"
    }
   },
   "source": [
    "# Import csv file from data folder\n",
    "data = pd.read_csv('../Data/Price.csv')\n",
    "\n",
    "# Explore data\n",
    "print(\"Columns of dataset:\", data.columns)\n",
    "print(\"Price Areas: \", data['PriceArea'].unique())\n",
    "\n",
    "# Filter all rows that do not equal DE for the column Country\n",
    "data = data[data['PriceArea'] == 'DK2'].reset_index(drop=True) # adjust for DK2\n",
    "data = data.drop(columns=['PriceDKK', 'PriceArea'])\n",
    "data = data.rename(columns={'HourDK': 'Date', 'PriceEUR': 'Price', 'HourUTC': 'Hour'})\n",
    "data['Date'] = pd.to_datetime(data['Date'], format='%d/%m/%Y').dt.strftime('%Y-%m-%d')  # Convert to standard format\n",
    "data['Hour'] = pd.to_timedelta(data['Hour']) + pd.Timedelta(hours=1) # Shift to UTC+1\n",
    "data['Hour'] = data['Hour'].apply(lambda x: x - pd.Timedelta(days=1) if x >= pd.Timedelta(days=1) else x) # Handles the 24:00:00 case\n",
    "data['Datetime'] = pd.to_datetime(data['Date']) + data['Hour']\n",
    "data['Hour'] = data['Hour'].apply(lambda x: str(x).split(' ')[-1])\n",
    "data['Hour'] = (pd.to_datetime(data['Hour'], format='%H:%M:%S')).dt.time\n",
    "data.head(25)"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Data/Price.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[52], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Import csv file from data folder\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m../Data/Price.csv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# Explore data\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumns of dataset:\u001B[39m\u001B[38;5;124m\"\u001B[39m, data\u001B[38;5;241m.\u001B[39mcolumns)\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001B[0m, in \u001B[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    209\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    210\u001B[0m         kwargs[new_arg_name] \u001B[38;5;241m=\u001B[39m new_arg_value\n\u001B[1;32m--> 211\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001B[0m, in \u001B[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m>\u001B[39m num_allow_args:\n\u001B[0;32m    326\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    327\u001B[0m         msg\u001B[38;5;241m.\u001B[39mformat(arguments\u001B[38;5;241m=\u001B[39m_format_argument_list(allow_args)),\n\u001B[0;32m    328\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[0;32m    329\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39mfind_stack_level(),\n\u001B[0;32m    330\u001B[0m     )\n\u001B[1;32m--> 331\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[0;32m    935\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m    936\u001B[0m     dialect,\n\u001B[0;32m    937\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    946\u001B[0m     defaults\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelimiter\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m},\n\u001B[0;32m    947\u001B[0m )\n\u001B[0;32m    948\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m--> 950\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    602\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[0;32m    604\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[1;32m--> 605\u001B[0m parser \u001B[38;5;241m=\u001B[39m \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    607\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[0;32m    608\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m   1439\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m   1441\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1442\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[1;34m(self, f, engine)\u001B[0m\n\u001B[0;32m   1733\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[0;32m   1734\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1735\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1736\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1737\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1738\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1739\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1740\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1741\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1742\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1743\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1744\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1745\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1746\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\common.py:856\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    851\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    852\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[0;32m    853\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[0;32m    854\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[0;32m    855\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[1;32m--> 856\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    857\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    858\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    859\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    860\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    861\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    862\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    863\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    864\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[0;32m    865\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '../Data/Price.csv'"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define seasons\n",
    "def get_season(date):\n",
    "    month = date.month\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    elif month in [9, 10, 11]:\n",
    "        return 'Fall'\n",
    "\n",
    "# Add a season column\n",
    "data['Season'] = data['Datetime'].apply(get_season)\n",
    "\n",
    "# Map seasons to colors\n",
    "season_colors = {'Winter': 'blue', 'Spring': 'green', 'Summer': 'orange', 'Fall': 'brown'}\n",
    "data['Color'] = data['Season'].map(season_colors)\n",
    "\n",
    "# Prepare data for LineCollection\n",
    "dates = data['Datetime'].values\n",
    "prices = data['Price'].values\n",
    "points = np.array([mdates.date2num(dates), prices]).T.reshape(-1, 1, 2)\n",
    "segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "colors = data['Color'].values[:-1]  # Create color array for segments\n",
    "\n",
    "# Create LineCollection\n",
    "lc = LineCollection(segments, colors=colors, linewidths=2)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(20, 5))\n",
    "ax.add_collection(lc)\n",
    "ax.set_xlim(dates.min(), dates.max())\n",
    "ax.set_ylim(prices.min(), prices.max())\n",
    "ax.set_title('Price Data by Season')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Price')\n",
    "\n",
    "# Format x-axis with proper dates\n",
    "ax.xaxis.set_major_locator(mdates.MonthLocator(interval=3))\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "fig.autofmt_xdate()\n",
    "\n",
    "# Create custom legend\n",
    "legend_handles = [mpatches.Patch(color=color, label=season) for season, color in season_colors.items()]\n",
    "ax.legend(handles=legend_handles, title='Season')\n",
    "plt.show()\n",
    "\n",
    "# Drop auxiliary columns\n",
    "data = data.drop(columns=['Season', 'Color'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Price categorization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def dynamic_categorization(df, column, time_column, window_size):\n",
    "    # Ensure the time column is sorted and in datetime format\n",
    "    df[time_column] = pd.to_datetime(df[time_column])\n",
    "    df = df.sort_values(by=[time_column, 'Hour'])\n",
    "\n",
    "    # Compute rolling statistics (mean+std)\n",
    "    df['RollingMean'] = df[column].rolling(window=window_size, min_periods=1).mean()\n",
    "    df['RollingStd'] = df[column].rolling(window=window_size, min_periods=1).std() # 2 values needed for std -> first row will always be NaN!\n",
    "\n",
    "    # Fill NaN std values with a small constant (e.g., 1) to avoid division errors\n",
    "    df['RollingStd'] = df['RollingStd'].fillna(1)\n",
    "\n",
    "    # Compute Z-scores\n",
    "    df['ZScore'] = (df[column] - df['RollingMean']) / df['RollingStd']\n",
    "\n",
    "    # Categorize based on Z-scores\n",
    "    bins = [-float('inf'), -1.5, -0.5, 0.5, 1.5, float('inf')]\n",
    "    labels = ['Very Low', 'Low', 'Normal', 'High', 'Very High']\n",
    "    df['Category'] = pd.cut(df['ZScore'], bins=bins, labels=labels)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply function to categorize\n",
    "categorized_data = dynamic_categorization(data, column='Price', time_column='Date', window_size=168)\n",
    "categorized_data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def plot_dynamic_categorization(df, column, time_column):\n",
    "    \"\"\"\n",
    "    Create a subplot with the dynamically categorized data scatter plot on the right\n",
    "    and line plots of the values for each dynamic category on the left.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input dataframe containing the categorized data.\n",
    "        column (str): The column to analyze and plot.\n",
    "        time_column (str): The time column to use for the x-axis.\n",
    "    \n",
    "    Returns:\n",
    "        None: Displays a subplot with the categorized data and category-based line plots.\n",
    "    \"\"\"\n",
    "    # Ensure time column is in datetime format and sorted\n",
    "    df[time_column] = pd.to_datetime(df[time_column])\n",
    "    df = df.sort_values(by=time_column)\n",
    "\n",
    "    # Get unique categories for dynamic labeling\n",
    "    unique_categories = df['Category'].dropna().unique()\n",
    "\n",
    "    # Set up subplots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6), gridspec_kw={'width_ratios': [2, 3]})\n",
    "\n",
    "    # Plot scatter plots of values for each dynamic category (left plot)\n",
    "    for category in unique_categories:\n",
    "        category_df = df[df['Category'] == category]\n",
    "        axes[0].scatter(category_df[time_column], category_df[column], label=category, alpha=0.7)\n",
    "    axes[0].set_title(f\"{column} Values for Each Dynamic Category:\")\n",
    "    axes[0].set_xlabel(\"Time\")\n",
    "    axes[0].set_ylabel(column)\n",
    "    axes[0].legend(title=\"Dynamic Categories\")\n",
    "    axes[0].grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "    # Plot scatter plot of dynamic categories over time (right plot)\n",
    "    axes[1].scatter(df[time_column], df['Category'], alpha=0.7, edgecolor='k')\n",
    "    axes[1].set_title(f\"{column} Dynamic Categories over Time:\")\n",
    "    axes[1].set_xlabel(\"Time\")\n",
    "    axes[1].set_ylabel(f\"Dynamic Categories of {column}\")\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    axes[1].grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Apply function to plot\n",
    "plot_dynamic_categorization(categorized_data, column='Price', time_column='Datetime')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition Matrix"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def calculate_transition_matrix(df, category_column=\"Category\"):\n",
    "    \"\"\"\n",
    "    Calculate the transition probability matrix for categories.\n",
    "\n",
    "    Args:\n",
    "    - df: DataFrame with a sequential column of categories.\n",
    "    - category_column: Name of the column containing the categories.\n",
    "\n",
    "    Returns:\n",
    "    - 4x4 transition probability matrix as a DataFrame.\n",
    "    \"\"\"\n",
    "    # Ensure the column exists\n",
    "    if category_column not in df.columns:\n",
    "        raise ValueError(f\"Column '{category_column}' not found in the DataFrame.\")\n",
    "\n",
    "    # Extract categories from input df\n",
    "\n",
    "    categories = df[category_column].unique()\n",
    "    transitions = df[category_column].values\n",
    "\n",
    "\n",
    "    # Initialize a 4x4 matrix for counts\n",
    "    transition_counts = pd.DataFrame(\n",
    "        np.zeros((len(categories), len(categories))),\n",
    "        index=categories,\n",
    "        columns=categories,\n",
    "    )\n",
    "\n",
    "    # Count transitions\n",
    "    for i in range(len(transitions) - 1):\n",
    "        current_category = transitions[i]\n",
    "        next_category = transitions[i + 1]\n",
    "        if current_category in categories and next_category in categories:\n",
    "            transition_counts.loc[current_category, next_category] += 1\n",
    "\n",
    "    # Normalize rows to get probabilities\n",
    "    transition_matrix = transition_counts.div(transition_counts.sum(axis=1), axis=0).fillna(0)\n",
    "\n",
    "    return transition_matrix\n",
    "\n",
    "# Calculate transition matrix\n",
    "transition_matrix = calculate_transition_matrix(categorized_data)\n",
    "transition_matrix"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plotting the transition matrix\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "cax = ax.matshow(transition_matrix.values, cmap='Blues', alpha=0.8)\n",
    "\n",
    "# Add text annotations\n",
    "for (i, j), val in np.ndenumerate(transition_matrix.values):\n",
    "    ax.text(j, i, f'{val:.2f}', ha='center', va='center', color='black')\n",
    "\n",
    "# Formatting the plot\n",
    "plt.xticks(range(len(transition_matrix.columns)), transition_matrix.columns, rotation=45)\n",
    "plt.yticks(range(len(transition_matrix.index)), transition_matrix.index)\n",
    "plt.title(\"Transition Matrix Visualization\", pad=20)\n",
    "plt.xlabel(\"To Category\")\n",
    "plt.ylabel(\"From Category\")\n",
    "plt.colorbar(cax)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDP Model (using the value iteration algorithm to define the best policy)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class BatteryValueIteration:\n",
    "    def __init__(self, soc_levels, actions, price_dict, price_series, price_categories, transition_matrix, gamma=0.9, epsilon=0.01, initial_SOC= 200):\n",
    "        \"\"\"\n",
    "        Initialize the Battery Value Iteration problem.\n",
    "        \n",
    "        Args:\n",
    "        - soc_levels: List of SOC levels (e.g., [0, 100, 200, ..., 500]).\n",
    "        - actions: List of actions (e.g., [\"Charge\", \"Discharge\", \"Nothing\"]).\n",
    "        - price_series: DataFrame with columns ['Hour', 'Category', 'Price'].\n",
    "        - price_categories: List of unique price categories (e.g., [\"Negative\", \"Low\", \"Medium\", \"High\"]).\n",
    "        - transition_matrix: Transition probability matrix (dict of dicts).\n",
    "        - gamma: Discount factor (default 0.9).\n",
    "        - epsilon: Convergence threshold for value iteration (default 0.01).\n",
    "        \"\"\"\n",
    "        self.soc_levels = soc_levels\n",
    "        self.actions = actions\n",
    "        self.price_dict = price_dict\n",
    "        self.price_cat_t = price_series['Category']\n",
    "        self.price_t = price_series['Price']\n",
    "        self.price_categories = price_categories\n",
    "        self.transition_matrix = transition_matrix.T  # Transpose for correct indexing\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Define all possible states as (SOC, PriceCategory)\n",
    "        self.states = [(soc, category) for soc in soc_levels for category in price_categories]\n",
    "        \n",
    "        # Initialize value function and policy\n",
    "        self.value_function = {state: 0 for state in self.states}\n",
    "        self.policy = {state: \"Nothing\" for state in self.states}\n",
    "\n",
    "        # Initialize state\n",
    "        self.init_state = (initial_SOC, self.price_cat_t[0])\n",
    "        print((initial_SOC, self.price_cat_t[0]))\n",
    "\n",
    "\n",
    "    def is_valid_action(self, soc, action):\n",
    "        \"\"\"Check if the action is valid given the SOC.\"\"\"\n",
    "        valid = not (\n",
    "            (action == \"Discharge\" and soc == 0) or\n",
    "            (action == \"Charge\" and soc == max(self.soc_levels))\n",
    "        )\n",
    "        #print(f\"Action: {action}, SOC: {soc}, Valid: {valid}\")  # Debugging action validity\n",
    "        return valid\n",
    "\n",
    "    def get_next_soc(self, soc, action):\n",
    "        \"\"\"Determine the next SOC based on the action.\"\"\"\n",
    "        if action == \"Charge\":\n",
    "            return min(soc + 100, max(self.soc_levels))\n",
    "        elif action == \"Discharge\":\n",
    "            return max(soc - 100, 0)\n",
    "        return soc\n",
    "\n",
    "    def reward(self, action, price):\n",
    "        \"\"\"Calculate the reward for a given action and price.\"\"\"\n",
    "        if action == \"Charge\":\n",
    "            return -100 * price  # Cost of charging\n",
    "        elif action == \"Discharge\":\n",
    "            return 100 * price  # Revenue from discharging\n",
    "        return 0  # No cost/reward for doing nothing\n",
    "\n",
    "    def value_iteration(self):\n",
    "        \"\"\"Perform the value iteration algorithm.\"\"\"\n",
    "        iteration = 0\n",
    "        self.deltas = []\n",
    "        \n",
    "        while True:\n",
    "            iteration += 1\n",
    "            delta = 0  # Track convergence\n",
    "            new_value_function = self.value_function.copy()\n",
    "\n",
    "            for state in self.states:\n",
    "                soc, price_category = state\n",
    "                action_values = []\n",
    "                action_feas = []\n",
    "\n",
    "                for action in self.actions:\n",
    "                    # Enforce validity check\n",
    "                    if not self.is_valid_action(soc, action):\n",
    "                        continue  ## to check\n",
    "                    \n",
    "                    #print(f\"Feasible action {state, action}\")\n",
    "                    next_soc = self.get_next_soc(soc, action)\n",
    "                    expected_value = 0\n",
    "\n",
    "                    for next_category, prob in self.transition_matrix[price_category].items():\n",
    "                        next_state = (next_soc, next_category)\n",
    "                        if next_state in self.states:\n",
    "                            reward = self.reward(action, self.price_dict[next_category])\n",
    "                            expected_value += prob * (reward + self.gamma * self.value_function[next_state])\n",
    "\n",
    "                    action_values.append(expected_value)\n",
    "                    action_feas.append(action)\n",
    "\n",
    "                # Debugging: Print state and valid actions\n",
    "                #print(f\"State: {state}, Valid Actions: {action_values}\")\n",
    "\n",
    "                # Update the value function and policy only if valid actions exist\n",
    "                if action_values:\n",
    "                    best_action_index = np.argmax(action_values)\n",
    "                    new_value_function[state] = action_values[best_action_index]\n",
    "                    self.policy[state] = action_feas[best_action_index]\n",
    "                else:\n",
    "                    # No valid actions; set default values\n",
    "                    new_value_function[state] = self.value_function[state]\n",
    "                    self.policy[state] = \"Nothing\"\n",
    "\n",
    "                delta = max(delta, abs(new_value_function[state] - self.value_function[state]))\n",
    "\n",
    "            self.value_function = new_value_function\n",
    "\n",
    "            # Display convergence status\n",
    "            #if iteration % 10 == 0:\n",
    "                #print(f\"Iteration {iteration}: Delta={delta}\")\n",
    "            self.deltas.append(delta)\n",
    "\n",
    "            # Check for convergence\n",
    "            if delta < self.epsilon:\n",
    "                break\n",
    "\n",
    "    def get_policy(self):\n",
    "        \"\"\"Return the computed policy.\"\"\"\n",
    "        return self.policy\n",
    "\n",
    "    def get_value_function(self):\n",
    "        \"\"\"Return the computed value function.\"\"\"\n",
    "        return self.value_function\n",
    "    \n",
    "    def test_on_data(self, price_series):\n",
    "        \"\"\"Test the policy on a given price series.\"\"\"\n",
    "        self.rewards = []\n",
    "        self.battery_actions = []\n",
    "        reward_cumul = 0\n",
    "        self.state = self.init_state\n",
    "        for i in range(len(price_series) - 1):\n",
    "            soc, price_category = self.state\n",
    "            action = self.policy[self.state]\n",
    "            reward = self.reward(action, price_series['Price'].iloc[i])\n",
    "            reward_cumul += reward\n",
    "            self.rewards.append(reward_cumul)\n",
    "            self.battery_actions.append((self.state, action, price_series['Price'].iloc[i], price_category))\n",
    "            next_soc = self.get_next_soc(soc, action)\n",
    "            self.state = (next_soc, price_series['Category'].iloc[i + 1])\n",
    "    \n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# SOC levels and actions\n",
    "soc_levels = [0, 100, 200, 300, 400, 500]\n",
    "# soc_levels = [0, 100]\n",
    "actions = [\"Charge\", \"Discharge\", \"Nothing\"]\n",
    "price_dict = {'Very Low': 0.1, 'Low': 0.3, 'Normal': 0.5, 'High': 0.7,  'Very High': 0.9}\n",
    "price_categories = ['Very Low', 'Low', 'Normal', 'High', 'Very High']\n",
    "\n",
    "# Initialize and run value iteration\n",
    "optimizer = BatteryValueIteration(soc_levels, actions, price_dict, categorized_data, price_categories, transition_matrix, gamma=0.99, epsilon=1e-6,initial_SOC=500)\n",
    "optimizer.value_iteration()\n",
    "optimizer.test_on_data(categorized_data)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "optimizer.battery_actions[:10]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"\\nPolicy:\")\n",
    "for state, action in optimizer.get_policy().items():\n",
    "    print(f\"State: {state}, Policy: {action}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"\\nValue Function:\")\n",
    "for state, value in optimizer.get_value_function().items():\n",
    "    print(f\"State: {state}, Value: {value:.2f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot the deltas over the iterations\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(optimizer.deltas[:], marker='o')\n",
    "plt.title(\"Value Iteration Convergence\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Delta\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T08:38:17.844848Z",
     "start_time": "2024-12-10T08:38:17.796781Z"
    }
   },
   "source": [
    "# Plot the rewards\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(optimizer.rewards)\n",
    "plt.title(\"Cumulative Rewards with Dynamic Pricing\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Cumulative Reward\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[40], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Plot the rewards\u001B[39;00m\n\u001B[0;32m      2\u001B[0m plt\u001B[38;5;241m.\u001B[39mfigure(figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m10\u001B[39m, \u001B[38;5;241m6\u001B[39m))\n\u001B[1;32m----> 3\u001B[0m plt\u001B[38;5;241m.\u001B[39mplot(\u001B[43moptimizer\u001B[49m\u001B[38;5;241m.\u001B[39mrewards)\n\u001B[0;32m      4\u001B[0m plt\u001B[38;5;241m.\u001B[39mtitle(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCumulative Rewards with Dynamic Pricing\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      5\u001B[0m plt\u001B[38;5;241m.\u001B[39mxlabel(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTime Step\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'optimizer' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDP Model (using the policy iteration algorithm to define the best policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Close Form  \n",
    "\n",
    "V = (I - γ * P)^-1 * R\n",
    "\n",
    "Where:\n",
    "- V: The value function (vector).\n",
    "- I: The identity matrix.\n",
    "- γ: The discount factor.\n",
    "- P: The state transition probability matrix.\n",
    "- R: The reward vector.\n",
    "\n",
    "\n",
    "This approach avoids iterative calculations by leveraging matrix operations for policy evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T08:38:17.952053Z",
     "start_time": "2024-12-10T08:38:17.901445Z"
    }
   },
   "source": [
    "class BatteryPolicyIteration:\n",
    "    def __init__(self, soc_levels, actions, price_dict, price_categories, transition_matrix, gamma=0.9):\n",
    "        self.soc_levels = soc_levels\n",
    "        self.actions = actions\n",
    "        self.price_dict = price_dict\n",
    "        self.price_categories = price_categories\n",
    "        self.transition_matrix = transition_matrix.T  # Transpose for correct indexing\n",
    "        self.gamma = gamma\n",
    "        self.states = [(soc, category) for soc in soc_levels for category in price_categories]\n",
    "        self.policy = {state: np.random.choice(actions) for state in self.states}\n",
    "        self.value_function = {state: 0 for state in self.states}\n",
    "        self.policy_convergence = []  # Track changes in the policy for convergence\n",
    "        self.value_convergence = []  # Track value function convergence\n",
    "        self.cumulative_rewards = []  # Track cumulative rewards during testing\n",
    "\n",
    "    def is_valid_action(self, soc, action):\n",
    "        \"\"\"Check if the action is valid given the SOC.\"\"\"\n",
    "        return not (\n",
    "            (action == \"Discharge\" and soc == 0) or\n",
    "            (action == \"Charge\" and soc == max(self.soc_levels))\n",
    "        )\n",
    "\n",
    "    def get_next_soc(self, soc, action):\n",
    "        \"\"\"Determine the next SOC based on the action.\"\"\"\n",
    "        if action == \"Charge\":\n",
    "            return min(soc + 100, max(self.soc_levels))\n",
    "        elif action == \"Discharge\":\n",
    "            return max(soc - 100, 0)\n",
    "        return soc\n",
    "\n",
    "    def reward(self, action, price):\n",
    "        \"\"\"Calculate the reward for a given action and price.\"\"\"\n",
    "        if action == \"Charge\":\n",
    "            return -100 * price\n",
    "        elif action == \"Discharge\":\n",
    "            return 100 * price\n",
    "        return 0\n",
    "\n",
    "    def _evaluate_policy_closed_form(self, policy):\n",
    "        \"\"\"\n",
    "        Evaluate the current policy using the closed-form solution for V = (I - gamma * P)^{-1} * R.\n",
    "        \"\"\"\n",
    "        num_states = len(self.states)\n",
    "        state_indices = {state: i for i, state in enumerate(self.states)}\n",
    "        transitions = np.zeros((num_states, num_states))\n",
    "        rewards = np.zeros(num_states)\n",
    "\n",
    "        for idx, state in enumerate(self.states):\n",
    "            soc, price_category = state\n",
    "            action = policy[state]\n",
    "\n",
    "            # Skip invalid actions\n",
    "            if not self.is_valid_action(soc, action):\n",
    "                continue\n",
    "\n",
    "            next_soc = self.get_next_soc(soc, action)\n",
    "            for next_category, prob in self.transition_matrix[price_category].items():\n",
    "                next_state = (next_soc, next_category)\n",
    "                if next_state in state_indices:\n",
    "                    next_idx = state_indices[next_state]\n",
    "                    transitions[idx, next_idx] += prob\n",
    "                    rewards[idx] += prob * self.reward(action, self.price_dict[next_category])\n",
    "\n",
    "        # Solve V = (I - gamma * P)^-1 * R\n",
    "        I = np.eye(num_states)\n",
    "        value_vector = np.linalg.inv(I - self.gamma * transitions).dot(rewards)\n",
    "\n",
    "        # Map the value vector back to the value function dictionary\n",
    "        value_function = {state: value_vector[i] for i, state in enumerate(self.states)}\n",
    "        return value_function\n",
    "\n",
    "    def policy_iteration(self):\n",
    "        \"\"\"\n",
    "        Perform policy iteration using the closed-form solution for policy evaluation.\n",
    "        Tracks policy and value convergence.\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            # Policy evaluation (closed-form)\n",
    "            old_value_function = self.value_function.copy()\n",
    "            value_function = self._evaluate_policy_closed_form(self.policy)\n",
    "\n",
    "            # Calculate value convergence (delta)\n",
    "            delta = max(abs(value_function[state] - old_value_function[state]) for state in self.states)\n",
    "            self.value_convergence.append(delta)\n",
    "\n",
    "            # Policy improvement\n",
    "            policy_stable = True\n",
    "            updated_policy = self.policy.copy()\n",
    "            changes = 0\n",
    "\n",
    "            for state in self.states:\n",
    "                soc, price_category = state\n",
    "                old_action = self.policy[state]\n",
    "                action_values = []\n",
    "                valid_actions = []\n",
    "\n",
    "                for action in self.actions:\n",
    "                    if self.is_valid_action(soc, action):\n",
    "                        next_soc = self.get_next_soc(soc, action)\n",
    "                        expected_value = 0\n",
    "\n",
    "                        for next_category, prob in self.transition_matrix[price_category].items():\n",
    "                            next_state = (next_soc, next_category)\n",
    "                            reward = self.reward(action, self.price_dict[next_category])\n",
    "                            expected_value += prob * (reward + self.gamma * value_function[next_state])\n",
    "\n",
    "                        action_values.append(expected_value)\n",
    "                        valid_actions.append(action)\n",
    "\n",
    "                if action_values:\n",
    "                    best_action = valid_actions[np.argmax(action_values)]\n",
    "                    updated_policy[state] = best_action\n",
    "                    if best_action != old_action:\n",
    "                        policy_stable = False\n",
    "                        changes += 1\n",
    "\n",
    "            self.policy = updated_policy\n",
    "            self.value_function = value_function\n",
    "            self.policy_convergence.append(changes)\n",
    "\n",
    "            # If policy is stable (no changes), break\n",
    "            if policy_stable:\n",
    "                break\n",
    "\n",
    "    def test_on_data(self, price_series, initial_soc=None):\n",
    "        \"\"\"Test the policy on a given price series.\"\"\"\n",
    "        self.rewards = []  # To track rewards at each step\n",
    "        self.cumulative_rewards = []  # To track cumulative rewards\n",
    "        cumulative_reward = 0\n",
    "        self.battery_actions = []  # To store actions and states\n",
    "\n",
    "        # Set the initial SOC (default to mid-level if not specified)\n",
    "        if initial_soc is None:\n",
    "            initial_soc = self.soc_levels[len(self.soc_levels) // 2]\n",
    "        self.state = (initial_soc, price_series['Category'].iloc[0])\n",
    "\n",
    "        for i in range(len(price_series) - 1):\n",
    "            soc, price_category = self.state\n",
    "\n",
    "            # Get the action from the policy\n",
    "            action = self.policy.get(self.state)  # Default to \"Nothing\" if state not in policy\n",
    "\n",
    "            # Calculate reward\n",
    "            price = price_series['Price'].iloc[i]\n",
    "            reward = self.reward(action, price)\n",
    "            cumulative_reward += reward\n",
    "\n",
    "            # Append rewards and cumulative rewards\n",
    "            self.rewards.append(reward)\n",
    "            self.cumulative_rewards.append(cumulative_reward)\n",
    "\n",
    "            # Track actions and states\n",
    "            self.battery_actions.append((self.state, action, price, price_category))\n",
    "\n",
    "            # Get the next SOC and update state\n",
    "            next_soc = self.get_next_soc(soc, action)\n",
    "            next_price_category = price_series['Category'].iloc[i + 1]\n",
    "            self.state = (next_soc, next_price_category)\n",
    "\n",
    "        print(\"Testing complete. Final cumulative reward:\", cumulative_reward)\n",
    "\n",
    "    def get_policy(self):\n",
    "        \"\"\"Return the computed policy.\"\"\"\n",
    "        return self.policy\n",
    "\n",
    "    def get_value_function(self):\n",
    "        \"\"\"Return the computed value function.\"\"\"\n",
    "        return self.value_function\n"
   ],
   "outputs": [],
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Iteration based\n",
    "### TO be supress but can be usefull if we had much more states and action and that iversion to have the value function can not work or is way slower"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T08:38:18.090608Z",
     "start_time": "2024-12-10T08:38:18.047606Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "class BatteryPolicyIteration2:\n",
    "    def __init__(self, soc_levels, actions, price_dict, price_categories, transition_matrix, gamma=0.9, epsilon=1e-6):\n",
    "        self.soc_levels = soc_levels\n",
    "        self.actions = actions\n",
    "        self.price_dict = price_dict\n",
    "        self.price_categories = price_categories\n",
    "        self.transition_matrix = transition_matrix.T\n",
    "        self.gamma = gamma\n",
    "        self.states = [(soc, category) for soc in soc_levels for category in price_categories]\n",
    "        self.policy = {state: np.random.choice(actions) for state in self.states}\n",
    "        self.value_function = {state: 0 for state in self.states}\n",
    "        self.convergence_deltas = []  # To track convergence during policy evaluation\n",
    "        self.epsilon = epsilon\n",
    "        self.cumulative_rewards = []  # To track cumulative rewards during testing\n",
    "\n",
    "    def is_valid_action(self, soc, action):\n",
    "        \"\"\"Check if the action is valid given the SOC.\"\"\"\n",
    "        return not (\n",
    "            (action == \"Discharge\" and soc == 0) or\n",
    "            (action == \"Charge\" and soc == max(self.soc_levels))\n",
    "        )\n",
    "\n",
    "    def get_next_soc(self, soc, action):\n",
    "        \"\"\"Determine the next SOC based on the action.\"\"\"\n",
    "        if action == \"Charge\":\n",
    "            return min(soc + 100, max(self.soc_levels))\n",
    "        elif action == \"Discharge\":\n",
    "            return max(soc - 100, 0)\n",
    "        return soc\n",
    "\n",
    "    def reward(self, action, price):\n",
    "        \"\"\"Calculate the reward for a given action and price.\"\"\"\n",
    "        if action == \"Charge\":\n",
    "            return -100 * price\n",
    "        elif action == \"Discharge\":\n",
    "            return 100 * price\n",
    "        return 0\n",
    "\n",
    "    def _evaluate_policy(self, policy):\n",
    "        \"\"\"\n",
    "        Evaluate the current policy to compute both the value function and the updated policy.\n",
    "        Returns:\n",
    "        - value_function: The computed value function.\n",
    "        - updated_policy: The improved policy.\n",
    "        - convergence_deltas: List of deltas for convergence tracking.\n",
    "        \"\"\"\n",
    "        value_function = {state: 0 for state in self.states}\n",
    "        updated_policy = policy.copy()\n",
    "        convergence_deltas = []\n",
    "\n",
    "        while True:\n",
    "            delta = 0\n",
    "            new_value_function = value_function.copy()\n",
    "\n",
    "            for state in self.states:\n",
    "                soc, price_category = state\n",
    "                best_action = None\n",
    "                best_value = float('-inf')\n",
    "\n",
    "                for action in self.actions:\n",
    "                    if not self.is_valid_action(soc, action):\n",
    "                        continue\n",
    "\n",
    "                    next_soc = self.get_next_soc(soc, action)\n",
    "                    expected_value = 0\n",
    "\n",
    "                    for next_category, prob in self.transition_matrix[price_category].items():\n",
    "                        next_state = (next_soc, next_category)\n",
    "                        reward = self.reward(action, self.price_dict[next_category])\n",
    "                        expected_value += prob * (reward + self.gamma * value_function[next_state])\n",
    "\n",
    "                    if expected_value > best_value:\n",
    "                        best_value = expected_value\n",
    "                        best_action = action\n",
    "\n",
    "                new_value_function[state] = best_value\n",
    "                updated_policy[state] = best_action\n",
    "                delta = max(delta, abs(new_value_function[state] - value_function[state]))\n",
    "\n",
    "            value_function = new_value_function\n",
    "            convergence_deltas.append(delta)\n",
    "\n",
    "            if delta < self.epsilon:\n",
    "                break\n",
    "\n",
    "        return value_function, updated_policy, convergence_deltas\n",
    "\n",
    "    def policy_iteration(self):\n",
    "        \"\"\"\n",
    "        Perform policy iteration using _evaluate_policy.\n",
    "        Returns:\n",
    "        - final_policy: The computed optimal policy.\n",
    "        - final_value_function: The value function corresponding to the optimal policy.\n",
    "        \"\"\"\n",
    "        policy = self.policy\n",
    "        while True:\n",
    "            value_function, updated_policy, deltas = self._evaluate_policy(policy)\n",
    "            if policy == updated_policy:  # If the policy does not change, we have converged.\n",
    "                break\n",
    "            policy = updated_policy\n",
    "\n",
    "        self.policy = policy\n",
    "        self.value_function = value_function\n",
    "        self.convergence_deltas = deltas\n",
    "\n",
    "    def test_on_data(self, price_series, initial_soc=None):\n",
    "        \"\"\"Test the policy on a given price series.\"\"\"\n",
    "        self.rewards = []  # To track rewards at each step\n",
    "        self.cumulative_rewards = []  # To track cumulative rewards\n",
    "        cumulative_reward = 0\n",
    "        self.battery_actions = []  # To store actions and states\n",
    "\n",
    "        # Set the initial SOC (default to mid-level if not specified)\n",
    "        if initial_soc is None:\n",
    "            initial_soc = self.soc_levels[len(self.soc_levels) // 2]\n",
    "        self.state = (initial_soc, price_series['Category'].iloc[0])\n",
    "\n",
    "        for i in range(len(price_series) - 1):\n",
    "            soc, price_category = self.state\n",
    "\n",
    "            # Get the action from the policy\n",
    "            action = self.policy.get(self.state)  # Default to \"Nothing\" if state not in policy\n",
    "\n",
    "            # Calculate reward\n",
    "            price = price_series['Price'].iloc[i]\n",
    "            reward = self.reward(action, price)\n",
    "            cumulative_reward += reward\n",
    "\n",
    "            # Append rewards and cumulative rewards\n",
    "            self.rewards.append(reward)\n",
    "            self.cumulative_rewards.append(cumulative_reward)\n",
    "\n",
    "            # Track actions and states\n",
    "            self.battery_actions.append((self.state, action, price, price_category))\n",
    "\n",
    "            # Get the next SOC and update state\n",
    "            next_soc = self.get_next_soc(soc, action)\n",
    "            next_price_category = price_series['Category'].iloc[i + 1]\n",
    "            self.state = (next_soc, next_price_category)\n",
    "\n",
    "        print(\"Testing complete. Final cumulative reward:\", cumulative_reward)\n",
    "\n",
    "    def get_policy(self):\n",
    "        \"\"\"Return the computed policy.\"\"\"\n",
    "        return self.policy\n",
    "\n",
    "    def get_value_function(self):\n",
    "        \"\"\"Return the computed value function.\"\"\"\n",
    "        return self.value_function"
   ],
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T08:38:18.185173Z",
     "start_time": "2024-12-10T08:38:18.120608Z"
    }
   },
   "source": [
    "# Policy Iteration\n",
    "\n",
    "soc_levels = [0, 100, 200, 300, 400, 500]\n",
    "actions = [\"Charge\", \"Discharge\", \"Nothing\"]\n",
    "price_dict = {'Very Low': 0.1, 'Low': 0.3, 'Normal': 0.5, 'High': 0.7,  'Very High': 0.9}\n",
    "price_categories = ['Very Low', 'Low', 'Normal', 'High', 'Very High']\n",
    "\n",
    "policy_optimizer = BatteryPolicyIteration(soc_levels, actions, price_dict, price_categories, transition_matrix, gamma=0.99)\n",
    "policy_optimizer.policy_iteration()\n",
    "\n",
    "# Compare Policies and Value Functions\n",
    "print(\"\\nPolicy Iteration - Policy:\")\n",
    "for state, action in policy_optimizer.get_policy().items():\n",
    "    print(f\"State: {state}, Policy: {action}\")\n",
    "\n",
    "print(\"\\nPolicy Iteration - Value Function:\")\n",
    "for state, value in policy_optimizer.get_value_function().items():\n",
    "    print(f\"State: {state}, Value: {value}\")\n"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transition_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[43], line 8\u001B[0m\n\u001B[0;32m      5\u001B[0m price_dict \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mVery Low\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m0.1\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLow\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m0.3\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNormal\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m0.5\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mHigh\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m0.7\u001B[39m,  \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mVery High\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m0.9\u001B[39m}\n\u001B[0;32m      6\u001B[0m price_categories \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mVery Low\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLow\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNormal\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mHigh\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mVery High\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m----> 8\u001B[0m policy_optimizer \u001B[38;5;241m=\u001B[39m BatteryPolicyIteration(soc_levels, actions, price_dict, price_categories, \u001B[43mtransition_matrix\u001B[49m, gamma\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.99\u001B[39m)\n\u001B[0;32m      9\u001B[0m policy_optimizer\u001B[38;5;241m.\u001B[39mpolicy_iteration()\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# Compare Policies and Value Functions\u001B[39;00m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'transition_matrix' is not defined"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Policy Iteration\n",
    "policy_optimizer2 = BatteryPolicyIteration2(soc_levels, actions, price_dict, price_categories, transition_matrix, gamma=0.99)\n",
    "policy_optimizer2.policy_iteration()\n",
    "\n",
    "# Compare Policies and Value Functions\n",
    "print(\"\\nPolicy Iteration - Policy:\")\n",
    "for state, action in policy_optimizer2.get_policy().items():\n",
    "    print(f\"State: {state}, Policy: {action}\")\n",
    "\n",
    "print(\"\\nPolicy Iteration - Value Function:\")\n",
    "for state, value in policy_optimizer2.get_value_function().items():\n",
    "    print(f\"State: {state}, Value: {value}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T08:38:18.293367Z",
     "start_time": "2024-12-10T08:38:18.229170Z"
    }
   },
   "source": [
    "# Plot cumulative rewards and delta convergence\n",
    "policy_optimizer.test_on_data(categorized_data,500)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot cumulative rewards\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(policy_optimizer.cumulative_rewards, label=\"Cumulative Reward\", color=\"green\")\n",
    "# plt.plot(policy_optimizer.rewards, label=\"Cumulative Reward\", color=\"red\")\n",
    "plt.title(\"Cumulative Reward Over Time\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Cumulative Reward\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Plot delta convergence\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(policy_optimizer.value_convergence, label=\"Delta Convergence\", color=\"blue\", marker='o')\n",
    "plt.title(\"Delta Convergence Over Iterations\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Delta\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'policy_optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[44], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Plot cumulative rewards and delta convergence\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[43mpolicy_optimizer\u001B[49m\u001B[38;5;241m.\u001B[39mtest_on_data(categorized_data,\u001B[38;5;241m500\u001B[39m)\n\u001B[0;32m      4\u001B[0m plt\u001B[38;5;241m.\u001B[39mfigure(figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m12\u001B[39m, \u001B[38;5;241m6\u001B[39m))\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# Plot cumulative rewards\u001B[39;00m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'policy_optimizer' is not defined"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison models"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Get policies from both methods\n",
    "policy_iteration_policy = policy_optimizer.get_policy()\n",
    "value_iteration_policy = optimizer.get_policy()\n",
    "\n",
    "# Compare the policies\n",
    "print(\"\\nComparison of Policies (Policy Iteration vs Value Iteration):\")\n",
    "mismatch_count = 0\n",
    "\n",
    "for state in policy_iteration_policy.keys():\n",
    "    pi_action = policy_iteration_policy[state]\n",
    "    vi_action = value_iteration_policy.get(state, \"Not Found\")\n",
    "\n",
    "    if pi_action != vi_action:\n",
    "        mismatch_count += 1\n",
    "        print(f\"State: {state} -> Policy Iteration: {pi_action}, Value Iteration: {vi_action}\")\n",
    "\n",
    "if mismatch_count == 0:\n",
    "    print(\"Both policies are identical.\")\n",
    "else:\n",
    "    print(f\"\\nTotal mismatches: {mismatch_count}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T08:38:18.402933Z",
     "start_time": "2024-12-10T08:38:18.349897Z"
    }
   },
   "source": [
    "optimizer.test_on_data(categorized_data)\n",
    "# Assuming policy_optimizer.test_on_data has been run successfully\n",
    "# Plot cumulative rewards and delta convergence\n",
    "policy_optimizer.test_on_data(categorized_data)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot cumulative rewards\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(policy_optimizer.cumulative_rewards, label=\"Cumulative Reward Policy\", color=\"green\")\n",
    "plt.plot(optimizer.rewards, label=\"Cumulative Reward Value\")\n",
    "plt.title(\"Cumulative Reward Over Time\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Cumulative Reward\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "# Plot delta convergence\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(policy_optimizer.value_convergence, label=\"Delta Convergence Policy\", color=\"green\", marker='o')\n",
    "plt.plot(optimizer.deltas, label=\"Delta Convergenc Value\", marker='o')\n",
    "plt.title(\"Delta Convergence Over Iterations\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Delta\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[45], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43moptimizer\u001B[49m\u001B[38;5;241m.\u001B[39mtest_on_data(categorized_data)\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# Assuming policy_optimizer.test_on_data has been run successfully\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Plot cumulative rewards and delta convergence\u001B[39;00m\n\u001B[0;32m      4\u001B[0m policy_optimizer\u001B[38;5;241m.\u001B[39mtest_on_data(categorized_data)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'optimizer' is not defined"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T08:38:18.481453Z",
     "start_time": "2024-12-10T08:38:18.408901Z"
    }
   },
   "source": [
    "from itertools import product\n",
    "\n",
    "def simplified_grid_search(data, param_grid):\n",
    "    \"\"\"\n",
    "    Perform grid search with selected parameters: rolling_window, gamma, num_classes, epsilon.\n",
    "\n",
    "    Parameters:\n",
    "    - data: Input dataset.\n",
    "    - param_grid: Dictionary of parameter ranges.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with sorted results by reward.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    param_names = list(param_grid.keys())\n",
    "    param_combinations = list(product(*param_grid.values()))\n",
    "\n",
    "    print(f\"Starting Simplified Grid Search over {len(param_combinations)} combinations...\")\n",
    "\n",
    "    for combination in param_combinations:\n",
    "        params = dict(zip(param_names, combination))\n",
    "        print(f\"Testing Config: {params}\")\n",
    "\n",
    "        # Perform dynamic categorization\n",
    "        categorized_data = dynamic_categorization(data.copy(), 'Price', 'Date', params['rolling_window'])\n",
    "        \n",
    "        # Bin and label the data\n",
    "        zscore_min = categorized_data['ZScore'].min()\n",
    "        zscore_max = categorized_data['ZScore'].max()\n",
    "        bins = np.linspace(zscore_min - 0.1, zscore_max + 0.1, params['num_classes'] + 1)\n",
    "        labels = [f\"Class_{i}\" for i in range(params['num_classes'])]\n",
    "        categorized_data['Category'] = pd.cut(categorized_data['ZScore'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "        # Skip configurations with invalid categories\n",
    "        if categorized_data['Category'].isna().any():\n",
    "            print(f\"Skipping due to NaN categories: {params}\")\n",
    "            continue\n",
    "\n",
    "        # Calculate transition matrix\n",
    "        transition_matrix = calculate_transition_matrix(categorized_data, 'Category')\n",
    "        if (transition_matrix.sum(axis=1) == 0).any():\n",
    "            print(f\"Skipping due to invalid transition matrix: {params}\")\n",
    "            continue\n",
    "\n",
    "        # Run optimization\n",
    "        soc_levels = [0, 100, 200, 300, 400, 500]\n",
    "        actions = [\"Charge\", \"Discharge\", \"Nothing\"]\n",
    "        price_dict = {label: i * 0.2 for i, label in enumerate(labels)}\n",
    "\n",
    "        policy_optimizer = BatteryPolicyIteration(soc_levels, \n",
    "                            actions, \n",
    "                            price_dict, \n",
    "                            labels, \n",
    "                            transition_matrix, \n",
    "                            gamma=0.99)\n",
    "\n",
    "        policy_optimizer.policy_iteration()\n",
    "        policy_optimizer.test_on_data(categorized_data)\n",
    "\n",
    "        final_reward = policy_optimizer.cumulative_rewards[-1] if optimizer.rewards else 0\n",
    "        results.append({**params, 'reward': final_reward})\n",
    "        print(f\"Reward: {final_reward:.2f}\")\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results).sort_values(by='reward', ascending=False)\n",
    "    print(\"\\nTop Configurations:\")\n",
    "    print(results_df.head(5))\n",
    "    return results_df\n",
    "\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'rolling_window': [12, 24, 48, 72],  # Vary rolling windows\n",
    "    'gamma': [0.98, 0.99, 0.999],  # Discount factor\n",
    "    'num_classes': [4, 5, 6],  # Number of classes\n",
    "    'epsilon': [0.05, 0.1, 0.2]  # Convergence threshold\n",
    "}\n",
    "\n",
    "# Run the grid search\n",
    "results_df = simplified_grid_search(data, param_grid)\n",
    "\n",
    "# Select top 5 configurations\n",
    "top_5_configs = results_df.head(5).to_dict(orient='records')\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(results_df['reward'], marker='o')\n",
    "plt.title(\"Rewards Across Configurations\")\n",
    "plt.xlabel(\"Configuration Index\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print top configurations\n",
    "print(\"Top Configurations by Reward:\")\n",
    "print(results_df.head(5))"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[46], line 80\u001B[0m\n\u001B[0;32m     72\u001B[0m param_grid \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m     73\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrolling_window\u001B[39m\u001B[38;5;124m'\u001B[39m: [\u001B[38;5;241m12\u001B[39m, \u001B[38;5;241m24\u001B[39m, \u001B[38;5;241m48\u001B[39m, \u001B[38;5;241m72\u001B[39m],  \u001B[38;5;66;03m# Vary rolling windows\u001B[39;00m\n\u001B[0;32m     74\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgamma\u001B[39m\u001B[38;5;124m'\u001B[39m: [\u001B[38;5;241m0.98\u001B[39m, \u001B[38;5;241m0.99\u001B[39m, \u001B[38;5;241m0.999\u001B[39m],  \u001B[38;5;66;03m# Discount factor\u001B[39;00m\n\u001B[0;32m     75\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnum_classes\u001B[39m\u001B[38;5;124m'\u001B[39m: [\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m, \u001B[38;5;241m6\u001B[39m],  \u001B[38;5;66;03m# Number of classes\u001B[39;00m\n\u001B[0;32m     76\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mepsilon\u001B[39m\u001B[38;5;124m'\u001B[39m: [\u001B[38;5;241m0.05\u001B[39m, \u001B[38;5;241m0.1\u001B[39m, \u001B[38;5;241m0.2\u001B[39m]  \u001B[38;5;66;03m# Convergence threshold\u001B[39;00m\n\u001B[0;32m     77\u001B[0m }\n\u001B[0;32m     79\u001B[0m \u001B[38;5;66;03m# Run the grid search\u001B[39;00m\n\u001B[1;32m---> 80\u001B[0m results_df \u001B[38;5;241m=\u001B[39m simplified_grid_search(\u001B[43mdata\u001B[49m, param_grid)\n\u001B[0;32m     82\u001B[0m \u001B[38;5;66;03m# Select top 5 configurations\u001B[39;00m\n\u001B[0;32m     83\u001B[0m top_5_configs \u001B[38;5;241m=\u001B[39m results_df\u001B[38;5;241m.\u001B[39mhead(\u001B[38;5;241m5\u001B[39m)\u001B[38;5;241m.\u001B[39mto_dict(orient\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrecords\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'data' is not defined"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# SOC levels and actions\n",
    "soc_levels = [0, 100, 200, 300, 400, 500]\n",
    "actions = [\"Charge\", \"Discharge\", \"Nothing\"]\n",
    "price_dict = {'Very Low': 0.1, 'Low': 0.3, 'Normal': 0.5, 'High': 0.7,  'Very High': 0.9}\n",
    "price_categories = ['Very Low', 'Low', 'Normal', 'High', 'Very High']\n",
    "\n",
    "# Initialize and run value iteration\n",
    "optimizer = BatteryValueIteration(soc_levels, actions, price_dict, categorized_data, price_categories, transition_matrix, gamma=0.99, epsilon=0.1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T08:38:18.527448Z",
     "start_time": "2024-12-10T08:38:18.511454Z"
    }
   },
   "source": [
    "class Expando(object):\n",
    "    '''\n",
    "        A small class which can have attributes set\n",
    "    '''\n",
    "    pass"
   ],
   "outputs": [],
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T08:38:18.619027Z",
     "start_time": "2024-12-10T08:38:18.592006Z"
    }
   },
   "source": [
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "\n",
    "class OptimisationModel:\n",
    "    def __init__(self, price_series, init_SOC=0, max_SOC=500, efficiency=1.0):\n",
    "        self.price_t = price_series['Price']\n",
    "        self.price_cat_t = price_series['Category']\n",
    "        self.time = len(price_series)\n",
    "        self.init_SOC = init_SOC\n",
    "        self.max_SOC = max_SOC\n",
    "        self.efficiency = efficiency  # Efficiency for charging/discharging\n",
    "        \n",
    "        self.results = {}\n",
    "        self._build_model()\n",
    "    \n",
    "    def _build_model(self):\n",
    "        self.m = gp.Model(\"Battery_Optimisation\")\n",
    "        \n",
    "        # Decision Variables\n",
    "        SOC = self.m.addVars(self.time, lb=0, ub=self.max_SOC, name=\"SOC\")  # SOC level\n",
    "        charge = self.m.addVars(self.time, lb=0, ub=1, vtype=GRB.BINARY, name=\"Charge\")  # Charge binary\n",
    "        discharge = self.m.addVars(self.time, lb=0, ub=1, vtype=GRB.BINARY, name=\"Discharge\")  # Discharge binary\n",
    "\n",
    "        # Constraints\n",
    "        self.m.addConstr(SOC[0] == self.init_SOC, \"InitialSOC\")\n",
    "        self.m.addConstrs(\n",
    "            (SOC[i] == SOC[i - 1] + 100 * (charge[i] * self.efficiency - discharge[i] / self.efficiency)\n",
    "             for i in range(1, self.time)), \"SOC_Balance\")\n",
    "        self.m.addConstrs(\n",
    "            (charge[i] + discharge[i] <= 1 for i in range(self.time)), \"NoSimultaneousChargeDischarge\")\n",
    "\n",
    "        # Objective: Maximize rewards\n",
    "        self.m.setObjective(\n",
    "            gp.quicksum((discharge[i] * self.price_t[i] - charge[i] * self.price_t[i]) * 100 for i in range(self.time)),\n",
    "            GRB.MAXIMIZE\n",
    "        )\n",
    "\n",
    "        # Solve\n",
    "        self.m.optimize()\n",
    "\n",
    "        # Store results\n",
    "        self.results[\"objective\"] = self.m.objVal\n",
    "        self.results[\"SOC\"] = [SOC[i].X for i in range(self.time)]\n",
    "        self.results[\"charge\"] = [charge[i].X for i in range(self.time)]\n",
    "        self.results[\"discharge\"] = [discharge[i].X for i in range(self.time)]\n",
    "        self.results[\"rewards\"] = self._compute_cumulative_rewards()\n",
    "\n",
    "    def _compute_cumulative_rewards(self):\n",
    "        \"\"\"Compute cumulative rewards from the optimization results.\"\"\"\n",
    "        cumulative_rewards = []\n",
    "        reward_cumul = 0\n",
    "        for t in range(self.time):\n",
    "            reward = (self.results[\"discharge\"][t] * self.price_t[t] -\n",
    "                      self.results[\"charge\"][t] * self.price_t[t]) * 100\n",
    "            reward_cumul += reward\n",
    "            cumulative_rewards.append(reward_cumul)\n",
    "        return cumulative_rewards\n"
   ],
   "outputs": [],
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T08:38:18.698609Z",
     "start_time": "2024-12-10T08:38:18.658609Z"
    }
   },
   "source": [
    "# Run the optimisation model\n",
    "model = OptimisationModel(categorized_data)"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'categorized_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[49], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Run the optimisation model\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m model \u001B[38;5;241m=\u001B[39m OptimisationModel(\u001B[43mcategorized_data\u001B[49m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'categorized_data' is not defined"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Compare Top 5 Configurations\n",
    "def compare_top_5_with_optimization(data, top_5_configs, optimization_rewards):\n",
    "    \"\"\"\n",
    "    Compare the rewards of the top 5 configurations with the optimization model reward over time.\n",
    "\n",
    "    Parameters:\n",
    "    - data: The dataset to use for categorization and testing.\n",
    "    - top_5_configs: List of the top 5 configurations from hyperparameter tuning.\n",
    "    - optimization_rewards: List of cumulative rewards from the optimization model.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary containing cumulative rewards and performance metrics for each top configuration.\n",
    "    \"\"\"\n",
    "    soc_levels = [0, 100, 200, 300, 400, 500]\n",
    "    actions = [\"Charge\", \"Discharge\", \"Nothing\"]\n",
    "    top_configs_results = {}  # Store results for each configuration\n",
    "\n",
    "    print(\"\\nComparing Top 5 Configurations with Optimization Model:\\n\")\n",
    "\n",
    "    for i, config in enumerate(top_5_configs):\n",
    "        rolling_window = config['rolling_window']\n",
    "        gamma = config['gamma']\n",
    "        num_classes = config['num_classes']\n",
    "        epsilon = config['epsilon']\n",
    "        print(f\"Processing Config {i + 1}: Rolling Window={rolling_window}, Gamma={gamma}, Num Classes={num_classes}\")\n",
    "\n",
    "        # Re-categorize the data with the specific rolling window\n",
    "        categorized_data = dynamic_categorization(data.copy(), 'Price', 'Date', rolling_window)\n",
    "        \n",
    "        # Create bins and labels for categorization\n",
    "        zscore_min = categorized_data['ZScore'].min()\n",
    "        zscore_max = categorized_data['ZScore'].max()\n",
    "        bins = np.linspace(zscore_min - 0.1, zscore_max + 0.1, num_classes + 1)\n",
    "        labels = [f\"Class_{i}\" for i in range(num_classes)]\n",
    "        categorized_data['Category'] = pd.cut(categorized_data['ZScore'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "        # Compute the transition matrix\n",
    "        transition_matrix = calculate_transition_matrix(categorized_data, 'Category')\n",
    "\n",
    "        # Initialize the optimizer with the specific gamma\n",
    "        optimizer = BatteryValueIteration(\n",
    "            soc_levels, actions, {label: i * 0.2 for i, label in enumerate(labels)},\n",
    "            categorized_data, labels, transition_matrix, gamma=gamma, epsilon=epsilon\n",
    "        )\n",
    "        optimizer.value_iteration()\n",
    "        optimizer.test_on_data(categorized_data)\n",
    "\n",
    "        # Collect cumulative rewards for this configuration\n",
    "        rewards = optimizer.rewards\n",
    "\n",
    "        # Calculate percentage of total optimization reward\n",
    "        final_reward_optimizer = rewards[-1] if rewards else 0\n",
    "        final_reward_optimization = optimization_rewards[-1] if optimization_rewards else 1  # Avoid division by zero\n",
    "        percentage_of_total = (final_reward_optimizer / final_reward_optimization) * 100\n",
    "\n",
    "        print(f\"Config {i + 1}:\")\n",
    "        print(f\"  Final Reward: {final_reward_optimizer:.2f}\")\n",
    "        print(f\"  Percentage of Optimal Reward: {percentage_of_total:.2f}%\\n\")\n",
    "\n",
    "        # Save results\n",
    "        top_configs_results[f\"Config {i + 1}\"] = {\n",
    "            \"rewards\": rewards,\n",
    "            \"final_reward\": final_reward_optimizer,\n",
    "            \"percentage_of_optimal\": percentage_of_total,\n",
    "        }\n",
    "\n",
    "    return top_configs_results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Run the updated comparison function\n",
    "top_configs_results = compare_top_5_with_optimization(data, top_5_configs, model.results[\"rewards\"])\n",
    "\n",
    "# Display the results\n",
    "for config_name, results in top_configs_results.items():\n",
    "    print(f\"{config_name} Results:\")\n",
    "    print(f\"  - Final Reward: {results['final_reward']:.2f}\")\n",
    "    print(f\"  - Percentage of Optimal Reward: {results['percentage_of_optimal']:.2f}%\")\n",
    "\n",
    "# Plot cumulative rewards for top 5 configurations and the optimization model\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.plot(model.results[\"rewards\"], label=\"Optimization Model\", linewidth=2, linestyle=\"--\")\n",
    "\n",
    "# Add top 5 configurations to the plot\n",
    "for config_name, results in top_configs_results.items():\n",
    "    plt.plot(results['rewards'], label=config_name, linewidth=1)\n",
    "\n",
    "# Formatting the plot\n",
    "plt.title(\"Cumulative Rewards Comparison: Top 5 Configurations vs Optimization Model\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Cumulative Reward\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T08:38:18.824184Z",
     "start_time": "2024-12-10T08:38:18.757197Z"
    }
   },
   "source": [
    "# Print the cumulative rewards of the optimisation model vs. the reinforcement learning model\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(optimizer.rewards, label=\"Reinforcement Learning Value\")\n",
    "plt.plot(policy_optimizer.cumulative_rewards, label=\"Reinforcement Learning Policy\")\n",
    "plt.plot(model.results[\"rewards\"], label=\"Optimisation Model\")\n",
    "plt.title(\"Cumulative Rewards Comparison\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Cumulative Reward\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[50], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Print the cumulative rewards of the optimisation model vs. the reinforcement learning model\u001B[39;00m\n\u001B[0;32m      2\u001B[0m plt\u001B[38;5;241m.\u001B[39mfigure(figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m12\u001B[39m, \u001B[38;5;241m6\u001B[39m))\n\u001B[1;32m----> 3\u001B[0m plt\u001B[38;5;241m.\u001B[39mplot(\u001B[43moptimizer\u001B[49m\u001B[38;5;241m.\u001B[39mrewards, label\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReinforcement Learning Value\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      4\u001B[0m plt\u001B[38;5;241m.\u001B[39mplot(policy_optimizer\u001B[38;5;241m.\u001B[39mcumulative_rewards, label\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReinforcement Learning Policy\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      5\u001B[0m plt\u001B[38;5;241m.\u001B[39mplot(model\u001B[38;5;241m.\u001B[39mresults[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrewards\u001B[39m\u001B[38;5;124m\"\u001B[39m], label\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOptimisation Model\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'optimizer' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pourcerntage_of_total = optimizer.rewards[len(optimizer.rewards)-1]/model.results[\"rewards\"][len(model.results[\"rewards\"])-1]\n",
    "print(\"The Reinforcement Learning model has achieved\", pourcerntage_of_total*100, \"% of the total reward of the Optimisation Model\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
